{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3553c6d-3398-4813-b769-35df5486c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### top  \n",
    "\n",
    "### 000 packages \n",
    "\n",
    "owner=\"\"\n",
    "###econiomics DASHBOARD\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display, HTML\n",
    "#import Haver\n",
    "#Haver.path(Haver.direct(1))    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import os\n",
    "def clear():\n",
    "    os.system('cls')\n",
    "# then you can clear the window with:\n",
    "from matplotlib.dates import date2num\n",
    "import os\n",
    "os.chdir('C:/database/spreadsheet/')\n",
    "#C:\\database\\spreadsheet\n",
    "#C:\\database\\dashboard\n",
    "print(os.getcwd())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.ticker as plticker\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import matplotlib.pyplot as pit\n",
    "from PIL import Image\n",
    "import time\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import calendar\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import warnings\n",
    "# Ignore all warning messages\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "### 000 comments\n",
    "\n",
    "make it both grppahn and a wayt ot play with data. \n",
    "\n",
    "\n",
    "ois analytics\n",
    "table meeting, enxt meetigns effective meetings, days tot that meeting s. \n",
    "Grpahs show hsitircla pricning of meetings. \n",
    "\n",
    "(1)Table summary with retusl . \n",
    "(1.1) udentify cyurrent fed meeting sdates\n",
    "(1.2) dataframe with meetings and deposite rate\n",
    "(2, ) Curves)\n",
    "2.1 Curves 3,\n",
    "2.2 Accumulates\n",
    "2.3 flies, \n",
    "Calucalte z scores 1month, 3 mnths  \n",
    "3. Table summary  \n",
    "4. Hisotigbram  \n",
    "\n",
    "\n",
    "make it dcovnert to frn atra of anbnuebnceten next date to date of polcy rate effectivfes. \n",
    "\n",
    "Histoghram \n",
    "hostircal meetings priceing s\n",
    "hisotiircal z socres, many deinfitions \n",
    "cotrelations heatmnaposwith USD meetign s\n",
    "z score 1 mon \n",
    "z score 3 mnts\n",
    "time of cliosiogns london 4pm \n",
    "shwo table iwht the dates. \n",
    "\n",
    "\n",
    "2 meetings curves\n",
    "3 meetings curves\n",
    "curve fly \n",
    "\n",
    "table summary \n",
    "Meetings. \n",
    "Curve,1 \n",
    "Curves 2 \n",
    "CUreves 3\n",
    "CUerves 4. \n",
    "Fly\n",
    "#Acucmualted#\n",
    "    \n",
    "\n",
    "Hisotigra, \n",
    "[0,1]\n",
    "[1,2]\n",
    "[2,3]\n",
    "[3,4]\n",
    "[4,5]\n",
    "[5,6]\n",
    "[6,7]\n",
    "\n",
    "Hisotirgrma of curves ,\n",
    "Hisotirgrma of flys \n",
    "\n",
    "i have  a dataframe panel table called \" central bank meetings\"\n",
    "that has the following columns\n",
    "country: ( examples USA, united kingdom\" Australia\"\n",
    "central ban: ( example , FED, BOE, RBA)\n",
    "year ( example  2026, 2025,2024,2023....\n",
    "month (  example, 1,3,4,6,7)\n",
    "day ( example 1,3,5,6,7,22,26,_\n",
    "countdown ( example - 30 , -5 , 100 , 120)\n",
    "\n",
    "for example thise table has  the fed central bank meeting of 2023-january, or the fed centrla bank meetign of june 2022\n",
    "the table has the the country, the central bank and the year, month and day of  the historical central bank meetings ( both past and future meetings)/ and the countdown is relative to today's date. ( central bank meeting data - today date). \n",
    "\n",
    "the other dataframe called  \"historical pricing\"  that is a daily time series with datetiem index. \n",
    "that has  following columns \n",
    "[ussofed1 bgn curncy,\tussofed2 bgn curncy,\tussofed3 bgn curncy,\tussofed4 bgn curncy,\tussofed5 bgn curncy,\tussofed6 bgn curncy,\tussofed7 bgn curncy,\tussofed8 bgn curncy]\n",
    "each column correspond to the historical pricing of a \"overnigth index swap\" or ois\n",
    "the logic that t this dataframe follows is the followings. \n",
    " at a specific date  ( row value) the central bank meeting that is more near in time has its data temporarily  in teh \"ussofed1 bgn curncy\" column ( this is the first meeting more near in time)\n",
    "at a specific date, the central bank meeting that is the  second meeting more near in time has its data temporarily  in teh \"ussofed2 bgn curncy\" column \n",
    "and the same logic follows ussofed8 bgn curncy.\n",
    "so untils the mopst near in time central bank meeting  happens, that specifc meeting will be stored in  ussofed1 bgn curncy.\n",
    "once the central bank meeting happens , in the next day we will ahave a new meeting as the ussofed1 bgn curncy \n",
    "( the second nearest meetign becomes the first nearest meeting after the first nearest meeting happens). \n",
    "so the rest of teh meetings fomes one position. \n",
    "\n",
    "\n",
    "I wnat ot devleop a logicv sin whcih i can as an output a dataframe that has as the columns all the  specifc meetinfs  in the \"centlera bank meetings\" table\n",
    "life for example \"fed-2024,jan\",\"fed 2024-march\", \"fed-2024 jun\" \n",
    "and in each column we have the historical pricing of that meetings ( in practice each historicaln meeting moves from  ussofed8 bgn curncy to ussofed1 bgn curncy in the \"historical pricing\" dataframe)\n",
    "\n",
    "\n",
    "i have  a dataframe panel table called \" central bank meetings\", that has panle data of several central banks with its historical central bank meetings with its respective dates\n",
    "each country only 1 central bank and multiple historical cenbtral bank meetings\n",
    "that has the following columns\n",
    "country: ( examples \"USA\", \"united kingdom\",\"eu\",\"Australia\",\"newzealand\")\n",
    "central ban: ( example , FED, BOE, ecb, RBA,rbnz)\n",
    "year ( example  2026, 2025,2024,2023,2022....\n",
    "month (  example, 1,3,4,6,7,8,11,12)\n",
    "day ( example 1,3,5,6,7,22,26,30_\n",
    "\n",
    "i want to calculate the time distnace between the meetings and today.  ( central banmk meetings date  - today)\n",
    "then group by central bank and i want to see the group of only upcmoing centrla bank meetings by central bank and sort them . \n",
    "the central bank meeting witht he smallest time different is the 1st upcoming meeting, then the secon centrla bank meeting data with the smalles is the 2nd upcoming meewting     \n",
    "for each centrla bank create a a new column  and add the correspoinding ticker () copsmidering the central bank adn the number of meeting , soindeinring the following groupm of ticker s\n",
    "fed_tickers correpsonds to fed\n",
    "the ticker  ussofed1 bgn curncy should correspodng to the first upcmoing meeting   \n",
    "                                                                                                                                         \n",
    "ecb_tickers correspondos to ecb\n",
    "the ticker eesf1a bgn curncy\",should correspodng to the first upcmoing meeting   \n",
    "                                                                                                                                         \n",
    "boe_tickers corresponds to boe\n",
    " the ticker gpsf1a bgn curncy  should correspodng to the first upcmoing meeting     \n",
    "                                                                                                                                         \n",
    "boc_tickers correpsonds to boc\n",
    " the ticker cdsf1a bgn curncy ,should correspodng to the first upcmoing meeting   \n",
    "                                                                                                                                         \n",
    "rba_tickers correpsonds to rba\n",
    "the ticker adsf1a curncy should correspodng to the first upcmoing meeting   \n",
    "                                                                                                                                         \n",
    "rbnz_tickers corrpsonds to rbnz     \n",
    "the ticker ndsf1a bgn curncy should correspodng to the first upcmoing meeting   \n",
    "                                                                                                                                         \n",
    "In teh end i wan a table for each central bank in whcih i ahve the centlra bank meeting,  adn the equivalent tickers from th elist of tickers. \n",
    "                                                                                                                                         \n",
    "\n",
    "\n",
    "\n",
    "### 000 parameters and lists\n",
    "\n",
    "# Data for the DataFrame\n",
    "data = [\n",
    "#    [\"us\", \"fdtr index\", 0],\n",
    "    [\"us\", \"fedl01 index\", 0],\n",
    "    [\"us\", \"ussofed1 bgn curncy\", 1],\n",
    "    [\"us\", \"ussofed2 bgn curncy\", 2],\n",
    "    [\"us\", \"ussofed3 bgn curncy\", 3],\n",
    "    [\"us\", \"ussofed4 bgn curncy\", 4],\n",
    "    [\"us\", \"ussofed5 bgn curncy\", 5],\n",
    "    [\"us\", \"ussofed6 bgn curncy\", 6],\n",
    "    [\"us\", \"ussofed7 bgn curncy\", 7],\n",
    "    [\"us\", \"ussofed8 bgn curncy\", 8],\n",
    "    [\"us\", \"ussofed9 bgn curncy\", 9],\n",
    "    [\"us\", \"ussofed10 bgn curncy\", 10],\n",
    "    [\"us\", \"ussofed11 bgn curncy\", 11],\n",
    "    [\"us\", \"ussofed12 bgn curncy\", 12],\n",
    "    [\"eu\", \"euordepo index\", 0],\n",
    "    [\"eu\", \"estron index\", 0],\n",
    "    [\"eu\", \"eesf1a bgn curncy\", 1],\n",
    "    [\"eu\", \"eesf2a bgn curncy\", 2],\n",
    "    [\"eu\", \"eesf3a bgn curncy\", 3],\n",
    "    [\"eu\", \"eesf4a bgn curncy\", 4],\n",
    "    [\"eu\", \"eesf5a bgn curncy\", 5],\n",
    "    [\"eu\", \"eesf6a bgn curncy\", 6],\n",
    "    [\"eu\", \"eesf7a bgn curncy\", 7],\n",
    "    [\"eu\", \"eesf8a bgn curncy\", 8],\n",
    "    [\"eu\", \"eesf9a bgn curncy\", 9],\n",
    "    [\"eu\", \"eesf10a bgn curncy\", 10],\n",
    "    [\"uk\", \"UKBRBASE IDNEX\", 0],\n",
    "    [\"uk\", \"SONIO/N INDEX\", 0],\n",
    "    [\"uk\", \"GPSF1A bgn curncy\", 1],\n",
    "    [\"uk\", \"GPSF2A bgn curncy\", 2],\n",
    "    [\"uk\", \"GPSF3A bgn curncy\", 3],\n",
    "    [\"uk\", \"GPSF4A bgn curncy\", 4],\n",
    "    [\"uk\", \"GPSF5A bgn curncy\", 5],\n",
    "    [\"uk\", \"GPSF6A bgn curncy\", 6],\n",
    "    [\"uk\", \"GPSF7A bgn curncy\", 7],\n",
    "    [\"uk\", \"GPSF8A bgn curncy\", 8],\n",
    "    [\"uk\", \"GPSF9A bgn curncy\", 9],\n",
    "    [\"uk\", \"GPSF10A bgn curncy\", 10],\n",
    "    [\"canada\", \"CAONREPO INDEX\", 0],\n",
    "    [\"canada\", \"CDSF1A bgn curncy\", 1],\n",
    "    [\"canada\", \"CDSF2A bgn curncy\", 2],\n",
    "    [\"canada\", \"CDSF3A bgn curncy\", 3],\n",
    "    [\"canada\", \"CDSF4A bgn curncy\", 4],\n",
    "    [\"canada\", \"CDSF5A bgn curncy\", 5],\n",
    "    [\"canada\", \"CDSF6A bgn curncy\", 6],\n",
    "    [\"canada\", \"CDSF7A bgn curncy\", 7],\n",
    "    [\"canada\", \"CDSF8A bgn curncy\", 8],\n",
    "    [\"canada\", \"CDSF9A bgn curncy\", 9],\n",
    "    [\"canada\", \"CDSF10A bgn curncy\", 10],\n",
    "    [\"australia\", \"RBACOR INDEX\", 0],\n",
    "    [\"australia\", \"ADSF1A Curncy\", 1],\n",
    "    [\"australia\", \"ADSF2A Curncy\", 2],\n",
    "    [\"australia\", \"ADSF3A Curncy\", 3],\n",
    "    [\"australia\", \"ADSF4A Curncy\", 4],\n",
    "    [\"australia\", \"ADSF5A Curncy\", 5],\n",
    "    [\"australia\", \"ADSF6A Curncy\", 6],\n",
    "    [\"australia\", \"ADSF7A Curncy\", 7],\n",
    "    [\"australia\", \"ADSF8A Curncy\", 8],\n",
    "    [\"australia\", \"ADSF9A Curncy\", 9],\n",
    "    [\"newzealand\", \"NZ0BCH Index\", 0],\n",
    "    [\"newzealand\", \"NZOCR Index\", 0],\n",
    "    [\"newzealand\", \"NDSF1A bgn curncy\", 1],\n",
    "    [\"newzealand\", \"NDSF2A bgn curncy\", 2],\n",
    "    [\"newzealand\", \"NDSF3A bgn curncy\", 3],\n",
    "    [\"newzealand\", \"NDSF4A bgn curncy\", 4],\n",
    "    [\"newzealand\", \"NDSF5A bgn curncy\", 5],\n",
    "    [\"newzealand\", \"NDSF6A bgn curncy\", 6],\n",
    "    [\"newzealand\", \"NDSF7A bgn curncy\", 7],\n",
    "    [\"newzealand\", \"NDSF8A bgn curncy\", 8],\n",
    "    [\"newzealand\", \"NDSF9A bgn curncy\", 9],\n",
    "    [\"newzealand\", \"NDSF10A bgn curncy\", 10],\n",
    "    [\"norway\", \"nobrdepa index\", 0],\n",
    "    [\"norway\", \"nksf1a bgn curncy\", 1],\n",
    "    [\"norway\", \"nksf2a bgn curncy\", 2],\n",
    "    [\"norway\", \"nksf3a bgn curncy\", 3],\n",
    "    [\"norway\", \"nksf4a bgn curncy\", 4],\n",
    "    [\"norway\", \"nksf5a bgn curncy\", 5],\n",
    "    [\"norway\", \"nksf6a bgn curncy\", 6],\n",
    "    [\"sweden\", \"swrratei index\", 0],\n",
    "    [\"sweden\", \"sksf1a curncy\", 1],\n",
    "    [\"sweden\", \"sksf2a curncy\", 2],\n",
    "    [\"sweden\", \"sksf3a curncy\", 3],\n",
    "    [\"sweden\", \"sksf4a curncy\", 4]\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "table000 = pd.DataFrame(data, columns=[\"country\", \"ticker\", \"number\"])\n",
    "table000=table000[table000.country == \"us\"]\n",
    "print(table000)\n",
    "#\"swrratei index\",\n",
    "#\"stib1d index\",\n",
    "#euordepo index\n",
    "#estron index\n",
    "#ukbrbase idnex\n",
    "#sonio/n index\n",
    "#cabrover index\n",
    "#caonrepo index\n",
    "#rbacor index\",\n",
    "#\"nz0bch index\",\n",
    "#nzocrs index\",\n",
    "#nobrdepa index\",\n",
    "#nowa index\",\n",
    "list_tickers= [\"ussofed1 bgn curncy\",\n",
    "               \"ussofed2 bgn curncy\",\n",
    "               \"ussofed3 bgn curncy\",\n",
    "               \"ussofed4 bgn curncy\",\n",
    "               \"ussofed5 bgn curncy\",\n",
    "               \"ussofed6 bgn curncy\",\n",
    "               \"ussofed7 bgn curncy\",\n",
    "               \"ussofed8 bgn curncy\",\n",
    "               \"ussofed9 bgn curncy\",\n",
    "               \"ussofed10 bgn curncy\",\n",
    "               \"ussofed11 bgn curncy\",\n",
    "               \"ussofed12 bgn curncy\",\n",
    "               \"eesf1a bgn curncy\",\n",
    "               \"eesf2a bgn curncy\",\n",
    "               \"eesf3a bgn curncy\",\n",
    "               \"eesf4a bgn curncy\",\n",
    "               \"eesf5a bgn curncy\",\n",
    "               \"eesf6a bgn curncy\",\n",
    "               \"eesf7a bgn curncy\",\n",
    "               \"eesf8a bgn curncy\",\n",
    "               \"eesf9a bgn curncy\",\n",
    "               \"eesf10a bgn curncy\",\n",
    "               \"gpsf1a bgn curncy\",\n",
    "               \"gpsf2a bgn curncy\",\n",
    "               \"gpsf3a bgn curncy\",\n",
    "               \"gpsf4a bgn curncy\",\n",
    "               \"gpsf5a bgn curncy\",\n",
    "               \"gpsf6a bgn curncy\",\n",
    "               \"gpsf7a bgn curncy\",\n",
    "               \"gpsf8a bgn curncy\",\n",
    "               \"gpsf9a bgn curncy\",\n",
    "               \"gpsf10a bgn curncy\",\n",
    "               \"cdsf1a bgn curncy\",\n",
    "               \"cdsf2a bgn curncy\",\n",
    "               \"cdsf3a bgn curncy\",\n",
    "               \"cdsf4a bgn curncy\",\n",
    "               \"cdsf5a bgn curncy\",\n",
    "               \"cdsf6a bgn curncy\",\n",
    "               \"cdsf7a bgn curncy\",\n",
    "               \"cdsf8a bgn curncy\",\n",
    "               \"cdsf9a bgn curncy\",\n",
    "               \"cdsf10a bgn curncy\",\n",
    "               \"adsf1a curncy\",\n",
    "               \"adsf2a curncy\",\n",
    "               \"adsf3a curncy\",\n",
    "               \"adsf4a curncy\",\n",
    "               \"adsf5a curncy\",\n",
    "               \"adsf6a curncy\",\n",
    "               \"adsf7a curncy\",\n",
    "               \"adsf8a curncy\",\n",
    "               \"adsf9a curncy\",\n",
    "               \"ndsf1a bgn curncy\",\n",
    "               \"ndsf2a bgn curncy\",\n",
    "               \"ndsf3a bgn curncy\",\n",
    "               \"ndsf4a bgn curncy\",\n",
    "               \"ndsf5a bgn curncy\",\n",
    "               \"ndsf6a bgn curncy\",\n",
    "               \"ndsf7a bgn curncy\",\n",
    "               \"ndsf8a bgn curncy\",\n",
    "               \"ndsf9a bgn curncy\",\n",
    "               \"ndsf10a bgn curncy\",\n",
    "               \"nksf1a bgn curncy\",\n",
    "                  \"nksf2a bgn curncy\",\n",
    "                  \"nksf3a bgn curncy\",\n",
    "                  \"nksf4a bgn curncy\",\n",
    "                  \"nksf5a bgn curncy\",\n",
    "                  \"nksf6a bgn curncy\",\n",
    "               \"sksf1a curncy\",\n",
    "                    \"sksf2a curncy\",\n",
    "                    \"sksf3a curncy\",\n",
    "                    \"sksf4a curncy\"]\n",
    "\n",
    "fed_tickers= [\"ussofed1 bgn curncy\",\n",
    "               \"ussofed2 bgn curncy\",\n",
    "               \"ussofed3 bgn curncy\",\n",
    "               \"ussofed4 bgn curncy\",\n",
    "               \"ussofed5 bgn curncy\",\n",
    "               \"ussofed6 bgn curncy\",\n",
    "               \"ussofed7 bgn curncy\",\n",
    "               \"ussofed8 bgn curncy\",\n",
    "               \"ussofed9 bgn curncy\",\n",
    "               \"ussofed10 bgn curncy\",\n",
    "               \"ussofed11 bgn curncy\",\n",
    "               \"ussofed12 bgn curncy\",]\n",
    "ecb_tickers = [\"eesf1a bgn curncy\",\n",
    "               \"eesf2a bgn curncy\",\n",
    "               \"eesf3a bgn curncy\",\n",
    "               \"eesf4a bgn curncy\",\n",
    "               \"eesf5a bgn curncy\",\n",
    "               \"eesf6a bgn curncy\",\n",
    "               \"eesf7a bgn curncy\",\n",
    "               \"eesf8a bgn curncy\",\n",
    "               \"eesf9a bgn curncy\",\n",
    "               \"eesf10a bgn curncy\",]\n",
    "boe_tickers = [\"gpsf1a bgn curncy\",\n",
    "               \"gpsf2a bgn curncy\",\n",
    "               \"gpsf3a bgn curncy\",\n",
    "               \"gpsf4a bgn curncy\",\n",
    "               \"gpsf5a bgn curncy\",\n",
    "               \"gpsf6a bgn curncy\",\n",
    "               \"gpsf7a bgn curncy\",\n",
    "               \"gpsf8a bgn curncy\",\n",
    "               \"gpsf9a bgn curncy\",\n",
    "               \"gpsf10a bgn curncy\",]\n",
    "boc_tickers = [\"cdsf1a bgn curncy\",\n",
    "               \"cdsf2a bgn curncy\",\n",
    "               \"cdsf3a bgn curncy\",\n",
    "               \"cdsf4a bgn curncy\",\n",
    "               \"cdsf5a bgn curncy\",\n",
    "               \"cdsf6a bgn curncy\",\n",
    "               \"cdsf7a bgn curncy\",\n",
    "               \"cdsf8a bgn curncy\",\n",
    "               \"cdsf9a bgn curncy\",\n",
    "               \"cdsf10a bgn curncy\",]\n",
    "rba_tickers = [\"adsf1a curncy\",\n",
    "               \"adsf2a curncy\",\n",
    "               \"adsf3a curncy\",\n",
    "               \"adsf4a curncy\",\n",
    "               \"adsf5a curncy\",\n",
    "               \"adsf6a curncy\",\n",
    "               \"adsf7a curncy\",\n",
    "               \"adsf8a curncy\",\n",
    "               \"adsf9a curncy\",]\n",
    "\n",
    "rbnz_tickers = [\"ndsf1a bgn curncy\",\n",
    "               \"ndsf2a bgn curncy\",\n",
    "               \"ndsf3a bgn curncy\",\n",
    "               \"ndsf4a bgn curncy\",\n",
    "               \"ndsf5a bgn curncy\",\n",
    "               \"ndsf6a bgn curncy\",\n",
    "               \"ndsf7a bgn curncy\",\n",
    "               \"ndsf8a bgn curncy\",\n",
    "               \"ndsf9a bgn curncy\",\n",
    "               \"ndsf10a bgn curncy\",]\n",
    "\n",
    "norges_tickers = [\"nksf1a bgn curncy\",\n",
    "                  \"nksf2a bgn curncy\",\n",
    "                  \"nksf3a bgn curncy\",\n",
    "                  \"nksf4a bgn curncy\",\n",
    "                  \"nksf5a bgn curncy\",\n",
    "                  \"nksf6a bgn curncy\"]\n",
    "              \n",
    "              \n",
    "riksbank_tickers = [\"sksf1a curncy\",\n",
    "                    \"sksf2a curncy\",\n",
    "                    \"sksf3a curncy\",\n",
    "                    \"sksf4a curncy\"]\n",
    "\n",
    "deposit_list = [\"fedl01 index\",\"estron index\",\n",
    "                \"ukbrbase index\",\"cabrover index\",\n",
    "                \"rbacor index\",\"nzocrs index\",\"nowa index\",\n",
    "                \"stib1d index\"]\n",
    "\n",
    "\n",
    "list000 = [ \"nksf1a bgn curncy\",\n",
    "           \"nksf2a bgn curncy\",\n",
    "           \"nksf3a bgn curncy\",\n",
    "           \"nksf4a bgn curncy\",\n",
    "           \"nksf5a bgn curncy\",\n",
    "           \"nksf6a bgn curncy\",\n",
    "           \"sksf1a curncy\",\n",
    "           \"sksf2a curncy\",\n",
    "           \"sksf3a curncy\",\n",
    "           \"sksf4a curncy\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1_code_ois = [\"USD.OIS\", \"EUR.ESTER\", \"GBP.SONIA\", \"AUD.AONIA\", \"NZD.NONIA\", \"CAD.CORRA\", \"CHF. SARON\"]\n",
    "# 1_ois = [\"USD\", \"EUR\", \"GBP\", \"AUD\", \"NZD\", \"CAD\", \"CHF\"]\n",
    "#    ois_table4g_001 = ois_table4g.copy()\n",
    "#    ois_table4g_001 = ois_table4g_001.tail(10)\n",
    "#    last_index_his = ois_table4g_001.index.max()\n",
    "#    last_index_his = last_index_his.date()\n",
    "\n",
    "#    ois_table4g_001 = ois_table4g_001[[(str(str(code_country_ois) + '_1')),\n",
    "#                                      (str(str(code_country_ois) + '_2')),\n",
    "#                                      (str(str(code_country_ois) + '_3')),\n",
    "#                                      (str(str(code_country_ois) + '_4')),\n",
    "#                                      (str(str(code_country_ois) + '_5')),\n",
    "#                                      (str(str(code_country_ois) + '_6')),\n",
    "#                                      (str(str(code_country_ois) + '_7')))]]\n",
    "\n",
    "\n",
    "print(\"Defining tickers for each central bank...\")\n",
    "\n",
    "\n",
    "###  001 load central bank meetings historical panel\n",
    "\n",
    "#df = pd.read_excel(\"arm_ois_data.xlsx\", sheet_name='data',header=0, index_col=0)\n",
    "#df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "#df.columns = df.columns.str.lower()\n",
    "#df=df[list_tickers]\n",
    "#print(df.shape[0])\n",
    "#print(df.shape[1])\n",
    "#print(df.head(5))\n",
    "#print(df.columns.tolist())\n",
    "\n",
    "cb000 = pd.read_excel(\"arm_ois_data.xlsx\", sheet_name='historical_dates',header=0)\n",
    "#df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "print(cb000.shape[0])\n",
    "print(cb000.shape[1])\n",
    "#print(df.head(5))\n",
    "print(cb000.columns.tolist())\n",
    "cb000=cb000[[\"country\",\"ccy\",\"cb\",\"year\",\"month\",\"day\"]]\n",
    "# Ensure columns are numeric\n",
    "cb000['year'] = pd.to_numeric(cb000['year'], errors='coerce')\n",
    "cb000['month'] = pd.to_numeric(cb000['month'], errors='coerce')\n",
    "cb000['day'] = pd.to_numeric(cb000['day'], errors='coerce')\n",
    "# Create a new column with the date in datetime format\n",
    "#cb000['date_meeting'] = pd.to_datetime(cb000[['year', 'month', 'day']], errors='coerce')\n",
    "## Add a column with today's date\n",
    "#cb000['today'] = pd.to_datetime(datetime.datetime.now().date())\n",
    "## Add a countdown column (difference in days)\n",
    "#cb000['countdown_meeting'] = (cb000['date_meeting'] - cb000['today']).dt.days\n",
    "## Compute meeting numbers without sorting\n",
    "#positive_indices = cb000[cb000['countdown_meeting'] >= 0].index\n",
    "#negative_indices = cb000[cb000['countdown_meeting'] < 0].index\n",
    "## Assign meeting numbers\n",
    "#meeting_numbers = [None] * len(cb000)\n",
    "## Assign positive meeting numbers\n",
    "#for i, idx in enumerate(positive_indices, start=1):\n",
    "#    meeting_numbers[idx] = i\n",
    "## Assign negative meeting numbers\n",
    "#for i, idx in enumerate(sorted(negative_indices, key=lambda x: cb000.loc[x, 'countdown_meeting'], reverse=True), start=0):\n",
    "#    meeting_numbers[idx] = -i\n",
    "## Add the meeting_number column to the DataFrame\n",
    "#cb000['meeting_number'] = meeting_numbers\n",
    "##rint(cb000)\n",
    "#cb001 = cb000[(cb000['meeting_number'] >= 0) & (cb000['meeting_number'] <= 12)]\n",
    "#print(cb001)\n",
    "#cb001 = pd.merge(cb001, table000, left_on='meeting_number', right_on='number', how='left')\n",
    "#print(cb001)\n",
    "\n",
    "central_bank_meetings=cb000.copy()\n",
    "# Step 1: Prepare central bank meetings table\n",
    "# Assuming `central_bank_meetings` dataframe exists\n",
    "print(\"Step 1: Preparing central bank meetings table...\")\n",
    "central_bank_meetings['date'] = pd.to_datetime(central_bank_meetings[['year', 'month', 'day']])\n",
    "# Create 'meeting_l' column in format 'CENTRALBANK-YYYYmon'\n",
    "central_bank_meetings['meeting_l'] = (\n",
    "    central_bank_meetings['cb'] + '-' +\n",
    "    central_bank_meetings['year'].astype(str) +\n",
    "    central_bank_meetings['month'].apply(lambda x: pd.to_datetime(x, format='%m').strftime('%b').lower())\n",
    ")\n",
    "\n",
    "# Create 'meeting_code' column in format 'centralbank-YYYY-MM'\n",
    "central_bank_meetings['meeting_code'] = (\n",
    "    central_bank_meetings['cb'].str.lower() + '-' +\n",
    "    central_bank_meetings['year'].astype(str) + '-' +\n",
    "    central_bank_meetings['month'].apply(lambda x: f\"{x:02d}\")\n",
    ")\n",
    "# Add today's date as a new column\n",
    "central_bank_meetings['today'] = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "central_bank_meetings['countdown'] = (central_bank_meetings['date'] - datetime.datetime.now()).dt.days\n",
    "\n",
    "# Filter for upcoming meetings only\n",
    "upcoming_meetings = central_bank_meetings[central_bank_meetings['countdown'] >= 0]\n",
    "\n",
    "# Sort meetings by countdown within each central bank\n",
    "grouped_meetings = upcoming_meetings.sort_values(by=['cb', 'countdown']).groupby('cb')\n",
    "\n",
    "# Define tickers for each central bank\n",
    "\n",
    "\n",
    "# Map tickers to meetings by central bank\n",
    "def assign_tickers(group, tickers):\n",
    "    group = group.copy()\n",
    "    group['ticker'] = tickers[:len(group)]\n",
    "    return group\n",
    "\n",
    "# Map tickers to meetings by central bank\n",
    "def assign_tickers(group, tickers):\n",
    "    group = group.copy()\n",
    "    # Adjust tickers to include None for the first row and fill with NaN if not enough tickers\n",
    "    assigned_tickers =  tickers[:len(group) - 1] + [np.nan] * max(0, len(group) - len(tickers) -1)\n",
    "    group['ticker'] = assigned_tickers[:len(group)]\n",
    "    return group\n",
    "    \n",
    "def assign_tickers(group, tickers):\n",
    "    group = group.copy()\n",
    "    # Ensure tickers match the group length\n",
    "    assigned_tickers = tickers[:len(group)] + [None] * max(0, len(group) - len(tickers))\n",
    "    group['ticker'] = assigned_tickers[:len(group)]\n",
    "    return group\n",
    "\n",
    "print(\"Assigning tickers to central bank meetings...\")\n",
    "final_meetings = pd.DataFrame()\n",
    "meetings_tables = {}\n",
    "for cb, tickers in zip(\n",
    "    ['fed', 'ecb', 'boe', 'boc', 'rba', 'rbnz','norges','riksbank'],\n",
    "    [fed_tickers, ecb_tickers, boe_tickers, boc_tickers, rba_tickers, rbnz_tickers,norges_tickers,riksbank_tickers]\n",
    "):\n",
    "    print(cb)\n",
    "    cb_meetings = grouped_meetings.get_group(cb)\n",
    "    cb_meetings = assign_tickers(cb_meetings, tickers)\n",
    "    final_meetings = pd.concat([final_meetings, cb_meetings], ignore_index=True)\n",
    "    meetings_tables[f\"{cb.lower()}_meetings\"] = cb_meetings\n",
    "\n",
    "# Step 4: Finalize the output\n",
    "print(\"Step 4: Finalizing and saving the output...\")\n",
    "final_meetings = final_meetings.sort_values(by=['cb', 'countdown'])\n",
    "final_meetings.to_parquet(\"upcoming_central_bank_meetings_with_tickers.parquet\")\n",
    "print(\"Processed and saved the upcoming central bank meetings with tickers successfully.\")\n",
    "\n",
    "# Create and display individual tables for each central bank\n",
    "for table_name, table_data in meetings_tables.items():\n",
    "    print(f\"{table_name}:\")\n",
    "    print(table_data)\n",
    "    globals()[table_name] = table_data\n",
    "\n",
    "\n",
    "######\" output: table with the last most rececnt meeting sdates and with the ticker s. \n",
    "print(\"Compelte\")\n",
    "\n",
    "\n",
    "\n",
    "print(fed_meetings)\n",
    "print(boc_meetings)\n",
    "print(ecb_meetings)\n",
    "print(boe_meetings)\n",
    "print(rba_meetings)\n",
    "print(rbnz_meetings)\n",
    "print(norges_meetings)\n",
    "print(riksbank_meetings)\n",
    "\n",
    "\n",
    "\n",
    "### 001 including previous meeting s \n",
    "\n",
    "central_bank_meetings=cb000.copy()\n",
    "# Step 1: Prepare central bank meetings table\n",
    "# Assuming `central_bank_meetings` dataframe exists\n",
    "print(\"Step 1: Preparing central bank meetings table...\")\n",
    "central_bank_meetings['date'] = pd.to_datetime(central_bank_meetings[['year', 'month', 'day']])\n",
    "# Create 'meeting_l' column in format 'CENTRALBANK-YYYYmon'\n",
    "central_bank_meetings['meeting_l'] = (\n",
    "    central_bank_meetings['cb'] + '-' +\n",
    "    central_bank_meetings['year'].astype(str) +\n",
    "    central_bank_meetings['month'].apply(lambda x: pd.to_datetime(x, format='%m').strftime('%b').lower())\n",
    ")\n",
    "\n",
    "# Create 'meeting_code' column in format 'centralbank-YYYY-MM'\n",
    "central_bank_meetings['meeting_code'] = (\n",
    "    central_bank_meetings['cb'].str.lower() + '-' +\n",
    "    central_bank_meetings['year'].astype(str) + '-' +\n",
    "    central_bank_meetings['month'].apply(lambda x: f\"{x:02d}\")\n",
    ")\n",
    "# Add today's date as a new column\n",
    "central_bank_meetings['today'] = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "central_bank_meetings['countdown'] = (central_bank_meetings['date'] - datetime.datetime.now()).dt.days\n",
    "\n",
    "print(\"Step: Including the smallest negative countdown for each 'cb' group...\")\n",
    "def filter_upcoming_and_nearest_negative(group):\n",
    "    # Identify rows with countdown >= 0\n",
    "    positive_or_zero = group[group['countdown'] >= 0]\n",
    "    # Find the row with the smallest negative countdown (if any)\n",
    "    smallest_negative = group[group['countdown'] < 0].nlargest(1, 'countdown')\n",
    "    # Combine both into one DataFrame\n",
    "    return pd.concat([positive_or_zero, smallest_negative], ignore_index=True)\n",
    "\n",
    "upcoming_meetings = central_bank_meetings.groupby('cb', group_keys=False).apply(filter_upcoming_and_nearest_negative)\n",
    "\n",
    "# Sort meetings by countdown within each central bank\n",
    "grouped_meetings = upcoming_meetings.sort_values(by=['cb', 'countdown']).groupby('cb')\n",
    "\n",
    "# Define tickers for each central bank\n",
    "\n",
    "\n",
    "\n",
    "# Map tickers to meetings by central bank\n",
    "def assign_tickers(group, tickers):\n",
    "    group = group.copy()\n",
    "    # Adjust tickers to include None for the first row and fill with NaN if not enough tickers\n",
    "    assigned_tickers = [None] + tickers[:len(group) - 1] + [np.nan] * max(0, len(group) - len(tickers) - 1)\n",
    "    group['ticker'] = assigned_tickers[:len(group)]\n",
    "    return group\n",
    "\n",
    "\n",
    "print(\"Assigning tickers to central bank meetings...\")\n",
    "final_meetings0 = pd.DataFrame()\n",
    "meetings_tables0 = {}\n",
    "for cb, tickers in zip(\n",
    "    ['fed', 'ecb', 'boe', 'boc', 'rba', 'rbnz','norges','riksbank'],\n",
    "    [fed_tickers, ecb_tickers, boe_tickers, boc_tickers, rba_tickers, rbnz_tickers,norges_tickers,riksbank_tickers]\n",
    "):\n",
    "    print(cb)\n",
    "    cb_meetings = grouped_meetings.get_group(cb)\n",
    "    cb_meetings = assign_tickers(cb_meetings, tickers)\n",
    "    final_meetings0 = pd.concat([final_meetings0, cb_meetings], ignore_index=True)\n",
    "    meetings_tables0[f\"{cb.lower()}_meetings0\"] = cb_meetings\n",
    "\n",
    "# Step 4: Finalize the output\n",
    "print(\"Step 4: Finalizing and saving the output...\")\n",
    "final_meetings0 = final_meetings0.sort_values(by=['cb', 'countdown'])\n",
    "final_meetings0.to_parquet(\"upcoming_central_bank_meetings_with_tickers0.parquet\")\n",
    "print(\"Processed and saved the upcoming central bank meetings with tickers successfully.\")\n",
    "\n",
    "# Create and display individual tables for each central bank\n",
    "for table_name, table_data in meetings_tables0.items():\n",
    "    print(f\"{table_name}:\")\n",
    "    print(table_data)\n",
    "    globals()[table_name] = table_data\n",
    "\n",
    "\n",
    "print(fed_meetings0)\n",
    "print(boc_meetings0)\n",
    "print(ecb_meetings0)\n",
    "print(boe_meetings0)\n",
    "print(rba_meetings0)\n",
    "print(rbnz_meetings0)\n",
    "print(norges_meetings0)\n",
    "print(riksbank_meetings0)\n",
    "\n",
    "#fed_meetings.to_clipboard()\n",
    "\n",
    "### 001 load central banks historical pricing data\n",
    "\n",
    "central_bank_meetings = pd.read_excel(\"arm_ois_data.xlsx\", sheet_name='historical_dates',header=0)\n",
    "#df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "print(central_bank_meetings.shape[0])\n",
    "print(central_bank_meetings.shape[1])\n",
    "#print(df.head(5))\n",
    "print(central_bank_meetings.columns.tolist())\n",
    "central_bank_meetings=central_bank_meetings[[\"country\",\"cb\",\"year\",\"month\",\"day\"]]\n",
    "# Ensure columns are numeric\n",
    "central_bank_meetings['year'] = pd.to_numeric(central_bank_meetings['year'], errors='coerce')\n",
    "central_bank_meetings['month'] = pd.to_numeric(central_bank_meetings['month'], errors='coerce')\n",
    "central_bank_meetings['day'] = pd.to_numeric(central_bank_meetings['day'], errors='coerce')\n",
    "print(central_bank_meetings)\n",
    "# Create a new column with the date in datetime format\n",
    "#cb000['date_meeting'] = pd.to_datetime(cb000[['year', 'month', 'day']], errors='coerce')\n",
    "# Add a column with today's date\n",
    "# Combine year, month, and day into a proper datetime column for central bank meetings\n",
    "central_bank_meetings[\"meeting_date\"] = pd.to_datetime(central_bank_meetings[[\"year\", \"month\", \"day\"]])\n",
    "central_bank_meetings['date'] = pd.to_datetime(central_bank_meetings[['year', 'month', 'day']])\n",
    "#central_bank_meetings['meeting_name'] = (\n",
    "#    central_bank_meetings['cb'] + '-' +\n",
    "#    central_bank_meetings['year'].astype(str) + '-' +\n",
    "#    central_bank_meetings['month'].apply(lambda x: pd.to_datetime(x, format='%m').strftime('%b').lower())\n",
    "#)\n",
    "central_bank_meetings['meeting_name'] = (\n",
    "    central_bank_meetings['cb'] + '-' +\n",
    "    central_bank_meetings['year'].astype(str) + '-' +\n",
    "    central_bank_meetings['month'].apply(lambda x: f\"{x:02d}\")\n",
    ")\n",
    "# Process: Create a column for each meeting in \"central bank meetings\"\n",
    "meeting_labels = central_bank_meetings.apply(\n",
    "    lambda row: f\"{row['cb']}-{row['year']}-{row['month']:02d}\", axis=1\n",
    ")\n",
    "central_bank_meetings[\"meeting_label\"] = meeting_labels\n",
    "print(\"Compelte\")\n",
    "\n",
    "print(central_bank_meetings)\n",
    "\n",
    "#central_bank_meetings.to_clipboard()\n",
    "\n",
    "### 001 load effective rate \n",
    "\n",
    "# Create a new DataFrame to hold the mapped pricing\n",
    "\n",
    "historical_deposit = pd.read_excel(\"arm_ois_data.xlsx\", sheet_name='data',header=0, index_col=0)\n",
    "historical_deposit.index = pd.to_datetime(historical_deposit.index, errors='coerce')\n",
    "historical_deposit.columns =historical_deposit.columns.str.lower()\n",
    "#### filtering jsut deposite rates\n",
    "historical_deposit=historical_deposit[deposit_list]\n",
    "historical_deposit.rename(columns={\"fedl01 index\": \"fed_effective\"}, inplace=True)\n",
    "historical_deposit.rename(columns={\"estron index\": \"ecb_effective\"}, inplace=True)\n",
    "historical_deposit.rename(columns={\"ukbrbase index\": \"boe_effective\"}, inplace=True)\n",
    "historical_deposit.rename(columns={\"cabrover index\": \"boc_effective\"}, inplace=True)\n",
    "historical_deposit.rename(columns={\"rbacor index\": \"rba_effective\"}, inplace=True)\n",
    "historical_deposit.rename(columns={\"nzocrs index\": \"rbnz_effective\"}, inplace=True)\n",
    "historical_deposit.rename(columns={\"nowa index\": \"norges_effective\"}, inplace=True)\n",
    "historical_deposit.rename(columns={\"stib1d index\": \"riksbank_effective\"}, inplace=True)\n",
    "#print(historical_deposit)\n",
    "print(\"Complete\")\n",
    "\n",
    "historical_pricing = pd.read_excel(\"arm_ois_data.xlsx\", sheet_name='data',header=0, index_col=0)\n",
    "historical_pricing.index = pd.to_datetime(historical_pricing.index, errors='coerce')\n",
    "historical_pricing.columns =historical_pricing.columns.str.lower()\n",
    "historical_pricing=historical_pricing[list_tickers]\n",
    "print(\"Complete\")\n",
    "\n",
    "index_date = pd.read_excel(\"arm_ois_data.xlsx\", sheet_name='data',header=0, index_col=0)\n",
    "print(index_date.index.max())\n",
    "index_date=index_date.index.max()\n",
    "index_date=index_date.date()\n",
    "print(index_date)\n",
    "timestamp_date=index_date\n",
    "\n",
    "### 002 computing historical fixed meeting logic\n",
    "\n",
    "#mapped_pricing = pd.DataFrame(index=historical_pricing.index, columns=meeting_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Initialize the historical pricing DataFrame\n",
    "print(\"Step 2: Initializing the historical pricing DataFrame...\")\n",
    "# Assuming `historical_pricing` DataFrame exists with a datetime index\n",
    "output_df = pd.DataFrame(index=historical_pricing.index)\n",
    "print(f\"Initialized output DataFrame with {len(output_df)} rows.\")\n",
    "\n",
    "# Step 3: Match meetings to tickers dynamically\n",
    "print(\"Step 3: Matching meetings to tickers dynamically...\")\n",
    "for date in historical_pricing.index:\n",
    "    #print(f\"Processing date: {date}\")\n",
    "    # Calculate countdowns for all meetings relative to the current date\n",
    "    central_bank_meetings['countdown'] = (central_bank_meetings['date'] - date).dt.days\n",
    "\n",
    "    # Filter for future meetings (countdown >= 0) and sort by countdown\n",
    "    future_meetings = central_bank_meetings[central_bank_meetings['countdown'] >= 0]\n",
    "    future_meetings = future_meetings.sort_values(by='countdown')\n",
    "\n",
    "    # Map tickers to meetings based on proximity and central bank\n",
    "    for cb, tickers in zip(\n",
    "        ['fed', 'ecb', 'boe', 'boc',\n",
    "         'rba', 'rbnz','norges','riksbank'],\n",
    "        [fed_tickers, ecb_tickers, boe_tickers, boc_tickers,\n",
    "         rba_tickers, rbnz_tickers,norges_tickers,riksbank_tickers]\n",
    "    ):\n",
    "        #print(cb)\n",
    "        cb_meetings = future_meetings[future_meetings['cb'] == cb].head(len(tickers))\n",
    "        for i, (_, meeting) in enumerate(cb_meetings.iterrows()):\n",
    "            ticker = tickers[i]\n",
    "            meeting_name = meeting['meeting_name']\n",
    "\n",
    "            # Add or update the column in the output dataframe\n",
    "            if meeting_name not in output_df.columns:\n",
    "                output_df[meeting_name] = np.nan\n",
    "\n",
    "            # Assign historical pricing data to the meeting column\n",
    "            \n",
    "            output_df.loc[date, meeting_name] = historical_pricing.loc[date, ticker]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Compelte\")\n",
    "\n",
    "### 002 saving\n",
    "\n",
    "print(\"Step 4: Finalizing the output DataFrame...\")\n",
    "output_df = output_df[sorted(output_df.columns)]\n",
    "output_df = output_df.sort_index()\n",
    "print(\"Output DataFrame finalized.\")\n",
    "#output_df.to_clipboard()\n",
    "# Save the result as Parquet\n",
    "output_df.to_parquet(\"central_bank_meetings_historical_pricing.parquet\")\n",
    "print(\"Processed and saved the output DataFrame successfully as Parquet.\")\n",
    "\n",
    "########################################################################\n",
    "\n",
    "### ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### changing dir \n",
    "\n",
    "os.chdir('C:/database/dashboard/')\n",
    "\n",
    "\n",
    "\n",
    "print(fed_meetings0)\n",
    "print(boc_meetings0)\n",
    "print(ecb_meetings0)\n",
    "print(boe_meetings0)\n",
    "print(rba_meetings0)\n",
    "print(rbnz_meetings0)\n",
    "print(norges_meetings0)\n",
    "print(riksbank_meetings0)\n",
    "\n",
    "from pandas.plotting import table\n",
    "cb_meeting_data=[\n",
    "    (\"usd\",\"fed\",\"fed_meetings0\",\"usd - OIS HISTORICAL\"),\n",
    "    (\"cad\",\"boc\",\"boc_meetings0\",\"CAD - OIS HISTORICAL\"),\n",
    "    (\"eur\",\"ecb\",\"ecb_meetings0\",\"EUR - OIS HISTORICAL\"),\n",
    "    (\"gbp\",\"boe\",\"boe_meetings0\",\"GBP - OIS HISTORICAL\"),\n",
    "    (\"aud\",\"rba\",\"rba_meetings0\",\"AUD - OIS HISTORICAL\"),\n",
    "    (\"nzd\",\"rbnz\",\"rbnz_meetings0\",\"NZD - OIS HISTORICAL\"),\n",
    "    (\"nok\",\"norges\",\"norges_meetings0\",\"NOK - OIS HISTORICAL\"),\n",
    "    (\"sek\",\"riksbank\",\"riksbank_meetings0\",\"SEK- OIS HISTORICAL\"),\n",
    "]\n",
    "\n",
    "\n",
    "# Process each dataframe\n",
    "for ccy, cb, bank, title in cb_meeting_data:\n",
    "    try:\n",
    "        print(f\"Processing dataframe: {bank}\")\n",
    "        \n",
    "        # Retrieve the dataframe\n",
    "        df = globals()[bank]\n",
    "        df = df.reset_index(drop=True)\n",
    "        df[\"meeting #\"]=df.index\n",
    "        df = df.drop(columns=['country'])\n",
    "        df = df.drop(columns=['ccy'])\n",
    "        #df[\"date\"] = df[\"date\"].date()\n",
    "        #df['date'] = pd.to_datetime(df['date'])\n",
    "        #df['date'] = df['date'].date()\n",
    "        # Create an image of the dataframe\n",
    "        fig, ax = plt.subplots(figsize=(16, 3))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Create a table plot\n",
    "        table_obj = table(ax, df, loc=\"center\", cellLoc=\"center\")\n",
    "        table_obj.auto_set_font_size(False)\n",
    "        table_obj.set_fontsize(11)\n",
    "        table_obj.scale(1.2, 1.2)                    \n",
    "        # Add a title\n",
    "        plt.title(title, fontsize=14, pad=20)\n",
    "        plt.savefig(f\"{ccy}_ois_summary_table.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.savefig(f\"{cb}_ois_summary_table.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Dataframe {bank} not found. Skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {bank}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 003 centra bank historical dataframe by country \n",
    "\n",
    "# Central banks to separate\n",
    "central_banks = [\"fed\", \"ecb\",\"boc\", \"boe\", \"rba\", \"rbnz\",\"norges\",\"riksbank\"]\n",
    "separated_dataframes = {}\n",
    "history_dataframes = {}\n",
    "\n",
    "for bank in central_banks:\n",
    "    bank_meetings = [col for col in output_df.columns if col.startswith(bank)]\n",
    "    dataframe_name = f\"{bank}_history\"\n",
    "    history_dataframes[dataframe_name] = output_df[bank_meetings]\n",
    "\n",
    "fed_history = history_dataframes['fed_history']\n",
    "ecb_history = history_dataframes['ecb_history']\n",
    "boe_history = history_dataframes['boe_history']\n",
    "boc_history = history_dataframes['boc_history']\n",
    "rba_history = history_dataframes['rba_history']\n",
    "rbnz_history = history_dataframes['rbnz_history']\n",
    "norges_history = history_dataframes['norges_history']\n",
    "riksbank_history = history_dataframes['riksbank_history']\n",
    "\n",
    "print(fed_history.shape[0])\n",
    "print(fed_history.shape[1])\n",
    "print(ecb_history.shape[0])\n",
    "print(ecb_history.shape[1])\n",
    "print(boe_history.shape[0])\n",
    "print(boe_history.shape[1])\n",
    "print(rba_history.shape[0])\n",
    "print(rba_history.shape[1])\n",
    "print(rbnz_history.shape[0])\n",
    "print(rbnz_history.shape[1])\n",
    "print(norges_history.shape[0])\n",
    "print(norges_history.shape[1])\n",
    "print(riksbank_history.shape[0])\n",
    "print(riksbank_history.shape[1])\n",
    "\n",
    "#norges_history.to_clipboard()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 003 centra bank historical dataframe  adding effective \n",
    "\n",
    "#['fedl01 index', 'estron index', 'ukbrbase index', 'cabrover index', 'rbacor index', 'nzocrs index', 'nowa index', 'stib1d index']\n",
    "#['fed_effective', 'ecb_effective', 'boe_effective', 'boc_effective', 'rba_effective', 'rbnz_effective', 'norges_effective', 'riksbank_effective']\n",
    "#fed_history\n",
    "print(historical_deposit.columns.tolist())\n",
    "\n",
    "# Perform a left join on the index\n",
    "print(fed_history.shape[0])\n",
    "print(fed_history.shape[1])\n",
    "fed_history = fed_history.join(historical_deposit[\"fed_effective\"], how=\"left\")\n",
    "print(fed_history.shape[0])\n",
    "print(fed_history.shape[1])\n",
    "\n",
    "print(ecb_history.shape[0])\n",
    "print(ecb_history.shape[1])\n",
    "ecb_history = ecb_history.join(historical_deposit[\"ecb_effective\"], how=\"left\")\n",
    "print(ecb_history.shape[0])\n",
    "print(ecb_history.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "print(boe_history.shape[0])\n",
    "print(boe_history.shape[1])\n",
    "boe_history = boe_history.join(historical_deposit[\"boe_effective\"], how=\"left\")\n",
    "print(boe_history.shape[0])\n",
    "print(boe_history.shape[1])\n",
    "\n",
    "\n",
    "print(boc_history.shape[0])\n",
    "print(boc_history.shape[1])\n",
    "boc_history = boc_history.join(historical_deposit[\"boc_effective\"], how=\"left\")\n",
    "print(boc_history.shape[0])\n",
    "print(boc_history.shape[1])\n",
    "\n",
    "\n",
    "print(rba_history.shape[0])\n",
    "print(rba_history.shape[1])\n",
    "rba_history = rba_history.join(historical_deposit[\"rba_effective\"], how=\"left\")\n",
    "print(rba_history.shape[0])\n",
    "print(rba_history.shape[1])\n",
    "\n",
    "\n",
    "print(rbnz_history.shape[0])\n",
    "print(rbnz_history.shape[1])\n",
    "rbnz_history = rbnz_history.join(historical_deposit[\"rbnz_effective\"], how=\"left\")\n",
    "print(rbnz_history.shape[0])\n",
    "print(rbnz_history.shape[1])\n",
    "\n",
    "\n",
    "print(norges_history.shape[0])\n",
    "print(norges_history.shape[1])\n",
    "norges_history = norges_history.join(historical_deposit[\"norges_effective\"], how=\"left\")\n",
    "print(norges_history.shape[0])\n",
    "print(norges_history.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "print(riksbank_history.shape[0])\n",
    "print(riksbank_history.shape[1])\n",
    "riksbank_history = riksbank_history.join(historical_deposit[\"riksbank_effective\"], how=\"left\")\n",
    "print(riksbank_history.shape[0])\n",
    "print(riksbank_history.shape[1])\n",
    "\n",
    "\n",
    "history_dataframes['fed_history']=fed_history\n",
    "history_dataframes['ecb_history']=ecb_history\n",
    "history_dataframes['boe_history']=boe_history\n",
    "history_dataframes['boc_history']=boc_history\n",
    "history_dataframes['rba_history']=rba_history\n",
    "history_dataframes['rbnz_history']=rbnz_history\n",
    "history_dataframes['norges_history']=norges_history\n",
    "history_dataframes['riksbank_history']=riksbank_history\n",
    "\n",
    "#boe_history.to_clipboard()\n",
    "#boe_history\n",
    "#fed_history.to_clipboard()\n",
    "\n",
    "\n",
    "### 003 Creating the hsitorical graphs  of rpicing by cb (daily)  with standard fed hisotry \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "dataframe_names = [\"fed_history\", \n",
    "                   \"boe_history\",\n",
    "                   \"boc_history\",\n",
    "                   \"ecb_history\", \n",
    "                   \"rba_history\",\n",
    "                   \"rbnz_history\",\n",
    "                   \"norges_history\",\n",
    "                   \"riksbank_history\"]\n",
    "\n",
    "for name in dataframe_names:\n",
    "    # Access the dataframe from the global environment\n",
    "    df = history_dataframes.get(name)\n",
    "    if df is not None and not df.empty:\n",
    "        # Set up color and line properties\n",
    "        num_lines = df.shape[1]\n",
    "        colors = cm.Blues(np.linspace(0.3, 1, num_lines))  # Gradient from light to dark blue\n",
    "        line_widths = np.linspace(0.5, 2, num_lines)  # Line widths from thin to thick\n",
    "\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(13, 6))\n",
    "        # Add the last line (rightmost column) in black\n",
    "        last_column = df.columns[-1]\n",
    "        print(last_column)\n",
    "        plt.plot(df.index, df[last_column], color=\"black\", linewidth=3,)\n",
    "        \n",
    "        # Plot all lines except the last column (rightmost)\n",
    "        for i, column in enumerate(df.columns[:-1]):  # Exclude the last column for black line\n",
    "            color = colors[i]\n",
    "            linewidth = line_widths[i]\n",
    "            plt.plot(df.index, df[column], color=color, linewidth=linewidth)\n",
    "\n",
    "        # Customize the plot\n",
    "        plt.title(f\"Time Series Plot for {name.replace('_history', '').upper()}\", fontsize=14)\n",
    "        plt.xlabel(\"Date\", fontsize=12)\n",
    "        plt.ylabel(\"Values\", fontsize=12)\n",
    "        plt.grid(True)\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot as PNG\n",
    "        output_path = f\"{name}.png\"\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "print(\"Plots saved with the last line in black and visible.\")\n",
    "\n",
    "### 003 makking effective after\n",
    "\n",
    "\n",
    "dataframes = [\n",
    "    (\"fed_history\", \"fed_effective\"),\n",
    "    (\"boe_history\", \"boe_effective\"),\n",
    "    (\"boc_history\", \"boc_effective\"),\n",
    "    (\"ecb_history\", \"ecb_effective\"),\n",
    "    (\"rba_history\", \"rba_effective\"),\n",
    "    (\"rbnz_history\", \"rbnz_effective\"),\n",
    "    (\"norges_history\", \"norges_effective\"),\n",
    "    (\"riksbank_history\", \"riksbank_effective\")\n",
    "]\n",
    "# Process each dataframe\n",
    "for df_name, effective_col in dataframes:\n",
    "    # Get the dataframe object\n",
    "    df = globals()[df_name]\n",
    "    \n",
    "    # Create a new dataframe to store the result\n",
    "    updated_df = df.copy()\n",
    "    \n",
    "    # Process each column except the effective column\n",
    "    for column in df.columns:\n",
    "        if column == effective_col:\n",
    "            continue\n",
    "        \n",
    "        # Iterate through the rows to check for NaNs\n",
    "        for idx in df.index:\n",
    "            if pd.isna(updated_df.loc[idx, column]):\n",
    "                # Replace NaN with the value from the effective column at the same index\n",
    "                updated_df.loc[idx, column] = updated_df.loc[idx, effective_col]\n",
    "    \n",
    "    # Create a new dataframe name by appending '_ef' to the original name\n",
    "    new_df_name = f\"{df_name}_ef\"\n",
    "    \n",
    "    # Assign the updated dataframe to the global namespace\n",
    "    globals()[new_df_name] = updated_df\n",
    "    \n",
    "    # Print confirmation\n",
    "    print(f\"Created new dataframe: {new_df_name}\")\n",
    "\n",
    "print(fed_history_ef.shape[0])\n",
    "print(fed_history_ef.shape[1])\n",
    "print(ecb_history_ef.shape[0])\n",
    "print(ecb_history_ef.shape[1])\n",
    "print(boe_history_ef.shape[0])\n",
    "print(boe_history_ef.shape[1])\n",
    "print(rba_history_ef.shape[0])\n",
    "print(rba_history_ef.shape[1])\n",
    "print(rbnz_history_ef.shape[0])\n",
    "print(rbnz_history_ef.shape[1])\n",
    "print(norges_history_ef.shape[0])\n",
    "print(norges_history_ef.shape[1])\n",
    "print(riksbank_history_ef.shape[0])\n",
    "print(riksbank_history_ef.shape[1])\n",
    "\n",
    "#fed_history_ef.to_clipboard()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 003 adding swiss cases\n",
    "\n",
    "### changing os \n",
    "\n",
    "os.chdir('C:/database/dashboard/')\n",
    "#C:\\database\\spreadsheet\n",
    "#C:\\database\\dashboard\n",
    "\n",
    "### 004 Creating analysis tables\n",
    "\n",
    "#print(fed_history)\n",
    "#print(fed_history)\n",
    "#print(boe_history)\n",
    "#print(boc_history)\n",
    "#print(ecb_history)\n",
    "#print(rba_history)\n",
    "#print(rbnz_history)\n",
    "#print(fed_meetings)\n",
    "#print(boc_meetings)\n",
    "#print(ecb_meetings)\n",
    "#print(boe_meetings)\n",
    "#print(rba_meetings)\n",
    "#print(rbnz_meetings)\n",
    "\n",
    "### 004 Creating analysis tables versio  effective  and  meeting sstartin gin 1. \n",
    "\n",
    "#fed_effective, meeting 1 meeting 2, meeting 3 meeting 4\n",
    "#tail \n",
    "\n",
    "meeting_data = [\n",
    "    (\"fed_meetings\", \"fed_history\", \"fed_effective\"),\n",
    "    (\"boc_meetings\", \"boc_history\", \"boc_effective\"),\n",
    "    (\"ecb_meetings\", \"ecb_history\", \"ecb_effective\"),\n",
    "    (\"boe_meetings\", \"boe_history\", \"boe_effective\"),\n",
    "    (\"rba_meetings\", \"rba_history\", \"rba_effective\"),\n",
    "    (\"rbnz_meetings\", \"rbnz_history\", \"rbnz_effective\"),\n",
    "    (\"norges_meetings\", \"norges_history\", \"norges_effective\"),\n",
    "    (\"riksbank_meetings\", \"riksbank_history\", \"riksbank_effective\"),\n",
    "]\n",
    "\n",
    "# Process each pair of meetings and history dataframes\n",
    "for meetings_df_name, history_df_name, effective_col in meeting_data:\n",
    "    # Retrieve the dataframes\n",
    "    meetings_df = globals()[meetings_df_name]\n",
    "    history_df = globals()[history_df_name]\n",
    "    \n",
    "    # Extract meeting codes\n",
    "    meeting_codes = meetings_df[\"meeting_code\"].tolist()\n",
    "    \n",
    "    # Filter columns that match meeting codes and include the effective column\n",
    "    columns_to_keep = [col for col in history_df.columns if col in meeting_codes]\n",
    "    columns_to_keep.append(effective_col)\n",
    "    \n",
    "    # Create a new dataframe with only the relevant columns\n",
    "    current_meetings_df = history_df[columns_to_keep]\n",
    "    \n",
    "    # Reorder columns to make the effective column the first\n",
    "    columns_reordered = [effective_col] + [col for col in current_meetings_df.columns if col != effective_col]\n",
    "    current_meetings_df = current_meetings_df[columns_reordered]\n",
    "    \n",
    "    # Keep only the last 100 rows\n",
    "    current_meetings_df = current_meetings_df.tail(100)\n",
    "    \n",
    "    # Assign the result to a new dataframe\n",
    "    new_df_name = f\"{history_df_name}_current\"\n",
    "    globals()[new_df_name] = current_meetings_df.copy()\n",
    "    \n",
    "    # Print confirmation\n",
    "    print(f\"Created dataframe: {new_df_name}\")\n",
    "\n",
    "\n",
    "\n",
    "#meeting_codes = fed_meetings[\"meeting_code\"].tolist()\n",
    "## Create a new dataframe with only the columns matching `meeting_code` and the `fed_effective` column\n",
    "#columns_to_keep = [col for col in fed_history.columns if col in meeting_codes]\n",
    "#columns_to_keep.append(\"fed_effective\")  # Include `fed_effective` column\n",
    "#fed_current_meetings = fed_history[columns_to_keep]\n",
    "## Reorder to make `fed_effective` the first column\n",
    "#columns_reordered = [\"fed_effective\"] + [col for col in fed_current_meetings.columns if col != \"fed_effective\"]\n",
    "#fed_current_meetings = fed_history[columns_reordered]\n",
    "#fed_current_meetings=fed_current_meetings.tail(100)\n",
    "#fed_current_meetings.head()\n",
    "\n",
    "print(fed_history_current)\n",
    "print(boc_history_current)\n",
    "print(ecb_history_current)\n",
    "print(boe_history_current)\n",
    "print(rba_history_current)\n",
    "print(rbnz_history_current)\n",
    "print(norges_history_current)\n",
    "print(riksbank_history_current)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 004 Creating analysis tables versio  000 effective  and  meeting sstartin gin  0 ( duplicaite with effectgive adn meetign 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 004 Makkign analysis tables \n",
    "\n",
    "meeting_data = [\n",
    "    (\"fed_meetings0\", \"fed_history_ef\", \"fed_effective\",\"fed_selected\"),\n",
    "    (\"boc_meetings0\", \"boc_history_ef\", \"boc_effective\",\"boc_selected\"),\n",
    "    (\"ecb_meetings0\", \"ecb_history_ef\", \"ecb_effective\",\"ecb_selected\"),\n",
    "    (\"boe_meetings0\", \"boe_history_ef\", \"boe_effective\",\"boe_selected\"),\n",
    "    (\"rba_meetings0\", \"rba_history_ef\", \"rba_effective\",\"rba_selected\"),\n",
    "    (\"rbnz_meetings0\", \"rbnz_history_ef\", \"rbnz_effective\",\"rbnz_selected\"),\n",
    "    (\"norges_meetings0\", \"norges_history_ef\", \"norges_effective\",\"norges_selected\"),\n",
    "    (\"riksbank_meetings0\", \"riksbank_history_ef\", \"riksbank_effective\",\"riksbank_selected\"),\n",
    "]\n",
    "\n",
    "# Process each pair of meetings and history dataframes\n",
    "for meetings_df_name, history_df_name, effective_col,df_selected in meeting_data:\n",
    "    # Retrieve the dataframes\n",
    "    print(meetings_df_name)\n",
    "    print(history_df_name)\n",
    "    meetings_df = globals()[meetings_df_name]\n",
    "    history_df = globals()[history_df_name]\n",
    "    print(meetings_df)\n",
    "    meeting_codes=meetings_df[\"meeting_code\"].tolist()\n",
    "    columns_to_keep = [col for col in history_df.columns if col in meeting_codes]\n",
    "    columns_to_keep.append(f\"{effective_col}\")  # Add the effective column\n",
    "    print(columns_to_keep)\n",
    "    history_df_selected = history_df[columns_to_keep]\n",
    "    columns_reordered = [f\"{effective_col}\"] + [col for col in history_df_selected.columns if col != f\"{effective_col}\"]\n",
    "    history_df_selected = history_df_selected[columns_reordered]\n",
    "    print(history_df_selected.columns.tolist())\n",
    "    history_df_selected = history_df_selected.tail(252)\n",
    "     # Create new column names\n",
    "    ois_columns = [\"effective\"] + [f\"ois_{i}\" for i in range(history_df_selected.shape[1] - 1)]\n",
    "    history_df_selected.columns = ois_columns\n",
    "    print(df_selected)\n",
    "    globals()[str(df_selected)] = history_df_selected.copy()\n",
    "        \n",
    "print(\"Complete\")\n",
    "\n",
    "#fed_selected.to_clipboard()\n",
    "\n",
    "#print(fed_analysis.head(5))\n",
    "\n",
    "### 004 analyts table loop \n",
    "\n",
    "parameter1_s=10\n",
    "parameter2_s=10\n",
    "\n",
    "meeting_data = [\n",
    "    (\"usd\",\"fed\",\"fed_selected\",\"fed_analysis\",\"USD OIS HISTORICAL\"),\n",
    "    (\"cad\",\"boc\",\"boc_selected\",\"boc_analysis\",\"CAD OIS HISTORICAL\"),\n",
    "    (\"eur\",\"ecb\",\"ecb_selected\",\"ecb_analysis\",\"EUR OIS HISTORICAL\"),\n",
    "    (\"gbp\",\"boe\",\"boe_selected\",\"boe_analysis\",\"GBP OIS HISTORICAL\"),\n",
    "    (\"aud\",\"rba\",\"rba_selected\",\"rba_analysis\",\"AUD OIS HISTORICAL\"),\n",
    "    (\"nzd\",\"rbnz\",\"rbnz_selected\",\"rbnz_analysis\",\"NZD OIS HISTORICAL\"),\n",
    "    (\"nok\",\"norges\",\"norges_selected\",\"norges_analysis\",\"NOK OIS HISTORICAL\"),\n",
    "    (\"sek\",\"riksbank\",\"riksbank_selected\",\"riksbank_analysis\",\"SEK OIS HISTORICAL\"),\n",
    "]\n",
    "\n",
    "\n",
    "df_analysis_agg = pd.DataFrame()\n",
    "    \n",
    "# Process each pair of meetings and history dataframes\n",
    "for ccy,cb, df_selected,df_analysis,text in meeting_data:\n",
    "    # Retrieve the dataframes\n",
    "    print(f\"Processing: {df_selected}\")\n",
    "    print(df_selected)\n",
    "    print(df_analysis)\n",
    "    name0 = cb\n",
    "    df_selected0 = globals()[df_selected]\n",
    "    df=df_selected0.copy()    \n",
    "    df_selected0 = globals()[df_selected]\n",
    "    df = df_selected0.copy()\n",
    "\n",
    "    # List of required columns\n",
    "    required_columns = [f\"ois_{i}\" for i in range(9)]\n",
    "    \n",
    "    # Ensure all required columns exist, fill missing columns with NaN\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Perform calculations, ensuring to handle NaN\n",
    "    df[\"ois_1_0\"] = df[\"ois_1\"] - df[\"ois_0\"]\n",
    "    df[\"ois_2_0\"] = df[\"ois_2\"] - df[\"ois_0\"]\n",
    "    df[\"ois_3_0\"] = df[\"ois_3\"] - df[\"ois_0\"]\n",
    "    df[\"ois_4_0\"] = df[\"ois_4\"] - df[\"ois_0\"]\n",
    "    df[\"ois_5_0\"] = df[\"ois_5\"] - df[\"ois_0\"]\n",
    "    df[\"ois_6_0\"] = df[\"ois_6\"] - df[\"ois_0\"]\n",
    "    df[\"ois_7_0\"] = df[\"ois_7\"] - df[\"ois_0\"]\n",
    "    df[\"ois_8_0\"] = df[\"ois_8\"] - df[\"ois_0\"]\n",
    "\n",
    "    df[\"ois_1_0\"] = df[\"ois_1\"] - df[\"ois_0\"]\n",
    "    df[\"ois_2_1\"] = df[\"ois_2\"] - df[\"ois_1\"]\n",
    "    df[\"ois_3_2\"] = df[\"ois_3\"] - df[\"ois_2\"]\n",
    "    df[\"ois_4_3\"] = df[\"ois_4\"] - df[\"ois_3\"]\n",
    "    df[\"ois_5_4\"] = df[\"ois_5\"] - df[\"ois_4\"]\n",
    "    df[\"ois_6_5\"] = df[\"ois_6\"] - df[\"ois_5\"]\n",
    "    df[\"ois_7_6\"] = df[\"ois_7\"] - df[\"ois_6\"]\n",
    "    df[\"ois_8_7\"] = df[\"ois_8\"] - df[\"ois_7\"]\n",
    "\n",
    "    df[\"ois_2_0\"] = df[\"ois_2\"] - df[\"ois_0\"]\n",
    "    df[\"ois_3_1\"] = df[\"ois_3\"] - df[\"ois_1\"]\n",
    "    df[\"ois_4_2\"] = df[\"ois_4\"] - df[\"ois_2\"]\n",
    "    df[\"ois_5_3\"] = df[\"ois_5\"] - df[\"ois_3\"]\n",
    "    df[\"ois_6_4\"] = df[\"ois_6\"] - df[\"ois_4\"]\n",
    "    df[\"ois_7_5\"] = df[\"ois_7\"] - df[\"ois_5\"]\n",
    "    df[\"ois_8_6\"] = df[\"ois_8\"] - df[\"ois_6\"]\n",
    "\n",
    "    df[\"ois_3_0\"] = df[\"ois_3\"] - df[\"ois_0\"]\n",
    "    df[\"ois_4_1\"] = df[\"ois_4\"] - df[\"ois_1\"]\n",
    "    df[\"ois_5_2\"] = df[\"ois_5\"] - df[\"ois_2\"]\n",
    "    df[\"ois_6_3\"] = df[\"ois_6\"] - df[\"ois_3\"]\n",
    "    df[\"ois_7_4\"] = df[\"ois_7\"] - df[\"ois_4\"]\n",
    "    df[\"ois_8_5\"] = df[\"ois_8\"] - df[\"ois_5\"]\n",
    "    \n",
    "    df[\"ois_4_0\"] = df[\"ois_4\"] - df[\"ois_0\"]\n",
    "    df[\"ois_5_1\"] = df[\"ois_5\"] - df[\"ois_1\"]\n",
    "    df[\"ois_6_2\"] = df[\"ois_6\"] - df[\"ois_2\"]\n",
    "    df[\"ois_7_3\"] = df[\"ois_7\"] - df[\"ois_3\"]\n",
    "    df[\"ois_8_4\"] = df[\"ois_8\"] - df[\"ois_4\"]\n",
    "\n",
    "    df[\"ois_1_2_3\"] = 2 * (df[\"ois_2\"]) - df[\"ois_1\"] - df[\"ois_3\"]\n",
    "    df[\"ois_2_3_4\"] = 2 * (df[\"ois_3\"]) - df[\"ois_2\"] - df[\"ois_4\"]\n",
    "    df[\"ois_3_4_5\"] = 2 * (df[\"ois_4\"]) - df[\"ois_3\"] - df[\"ois_5\"]\n",
    "    df[\"ois_4_5_6\"] = 2 * (df[\"ois_5\"]) - df[\"ois_4\"] - df[\"ois_6\"]\n",
    "    df[\"ois_5_6_7\"] = 2 * (df[\"ois_6\"]) - df[\"ois_5\"] - df[\"ois_7\"]\n",
    "    df[\"ois_6_7_8\"] = 2 * (df[\"ois_7\"]) - df[\"ois_6\"] - df[\"ois_8\"]\n",
    "\n",
    "    # List of OIS variables for rolling calculations\n",
    "    list_ois = [\"ois_0\",\"ois_1\",\"ois_2\", \"ois_3\", \"ois_4\", \"ois_5\", \"ois_6\", \"ois_7\", \"ois_8\",\n",
    "                \"ois_1_0\",\"ois_2_1\", \"ois_3_2\", \"ois_4_3\", \"ois_5_4\", \"ois_6_5\", \"ois_7_6\", \"ois_8_7\"]\n",
    "\n",
    "    # Perform rolling and lag calculations\n",
    "    for variable in list_ois:\n",
    "        #print(f\"Processing variable: {variable}\")\n",
    "        if variable not in df.columns:\n",
    "            df[variable] = np.nan  # Ensure variable exists as a column\n",
    "        df[f\"mav3d_{variable}\"] = df[variable].rolling(3).mean()\n",
    "        df[f\"mav1w_{variable}\"] = df[variable].rolling(5).mean()\n",
    "        df[f\"mav2w_{variable}\"] = df[variable].rolling(10).mean()\n",
    "        df[f\"mav3w_{variable}\"] = df[variable].rolling(15).mean()\n",
    "        df[f\"mav4w_{variable}\"] = df[variable].rolling(20).mean()\n",
    "        df[f\"mav1m_{variable}\"] = df[variable].rolling(22).mean()\n",
    "        df[f\"mav2m_{variable}\"] = df[variable].rolling(44).mean()\n",
    "        df[f\"mav3m_{variable}\"] = df[variable].rolling(66).mean()\n",
    "        df[f\"std1m_{variable}\"] = df[variable].rolling(22).std()\n",
    "        df[f\"std2m_{variable}\"] = df[variable].rolling(44).std()\n",
    "        df[f\"std3m_{variable}\"] = df[variable].rolling(66).std()\n",
    "        df[f\"lag1m_{variable}\"] = df[variable].shift(22)\n",
    "        df[f\"lag2m_{variable}\"] = df[variable].shift(44)\n",
    "        df[f\"lag3m_{variable}\"] = df[variable].shift(66)\n",
    "        df[f\"z1m_{variable}\"] = (df[variable] - df[f\"mav1m_{variable}\"]) / df[f\"std1m_{variable}\"]\n",
    "        df[f\"z2m_{variable}\"] = (df[variable] - df[f\"mav2m_{variable}\"]) / df[f\"std2m_{variable}\"]\n",
    "        df[f\"z3m_{variable}\"] = (df[variable] - df[f\"mav3m_{variable}\"]) / df[f\"std3m_{variable}\"]\n",
    "\n",
    "    # Save the resulting dataframe individual dataframe\n",
    "    globals()[df_analysis] = df.copy()\n",
    "\n",
    "\n",
    "    \n",
    "    ###merging all results in a single dataframe     \n",
    "    list_ois=[\"ois_0\",\"ois_1\",\"ois_2\",\"ois_3\",\"ois_4\",\"ois_5\",\"ois_6\",\"ois_7\",\"ois_8\",\n",
    "              \"ois_1_0\",\"ois_2_0\",\"ois_3_0\",\"ois_4_0\",\"ois_5_0\",\"ois_6_0\",\"ois_7_0\",\"ois_8_0\",\n",
    "    \"ois_2_1\",\"ois_3_2\",\"ois_4_3\",\"ois_5_4\",\"ois_6_5\",\"ois_7_6\",\"ois_8_7\",\n",
    "    \"ois_3_1\",\"ois_4_2\",\"ois_5_3\",\"ois_6_4\",\"ois_7_5\",\"ois_8_6\",\n",
    "    \"ois_4_1\",\"ois_5_2\",\"ois_6_3\",\"ois_7_4\",\"ois_8_5\",\n",
    "    \"ois_5_1\",\"ois_6_2\",\"ois_7_3\",\"ois_8_4\",\n",
    "     \"ois_1_2_3\",\"ois_2_3_4\",\"ois_3_4_5\",\"ois_4_5_6\",\"ois_5_6_7\",\"ois_6_7_8\",\n",
    "             \"z3m_ois_0\",\"z3m_ois_1\",\"z3m_ois_2\",\"z3m_ois_3\",\"z3m_ois_4\",\n",
    "             \"z3m_ois_5\",\"z3m_ois_6\",\"z3m_ois_7\",\"z3m_ois_8\"]\n",
    "    \n",
    "    df_agg= df.copy()\n",
    "    df_agg=df_agg[list_ois]\n",
    "    # Add the prefix to column names\n",
    "    df_agg.columns = [f\"{cb}_{col}\" if col != df.index.name else col for col in df_agg.columns]\n",
    "    # Concatenate the dataframe horizontally\n",
    "    df_analysis_agg = pd.concat([df_analysis_agg, df_agg], axis=1)    \n",
    "    del list_ois\n",
    "\n",
    "    list_ois=[\"ois_0\",\"ois_1\",\"ois_2\",\"ois_3\",\"ois_4\",\"ois_5\",\"ois_6\",\"ois_7\",\"ois_8\"]\n",
    "    # Perform rolling and lag calculations\n",
    "    for variable in list_ois:\n",
    "        #print(f\"Processing variable: {variable}\")\n",
    "        if variable in df.columns:\n",
    "            \n",
    "            title_r=f\"{cb}_{variable}\"\n",
    "            y_label=\"Index %\"\n",
    "            x_label=title_r\n",
    "            df1=df[variable]\n",
    "            df1=df1.tail(72)\n",
    "            g1=df1.copy()\n",
    "            fig, ax = plt.subplots(figsize=(parameter1_s, \n",
    "                                            parameter2_s))\n",
    "            g1.plot(kind='line', ax=ax,\n",
    "                    y=[variable], color=\"blue\", \n",
    "                    linewidth=3, linestyle='-',\n",
    "                    marker='P', markersize=1)\n",
    "        \n",
    "            #plt.axhline(0, color='black', lw=1, linestyle='dashed', alpha=0.75)\n",
    "            plt.style.use('default')\n",
    "            plt.ylabel(str(y_label), size=6)\n",
    "            plt.xlabel(str(x_label), size=6)\n",
    "            plt.tick_params(labelsize=2)\n",
    "            plt.title(str(title_r), size=7)\n",
    "            plt.grid(axis='both', alpha=.3)\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.set_ylim(ax.get_ylim())\n",
    "            plt.text(0.15, 0.15, str(variable), fontsize=40, color='gray', alpha=0.5,\n",
    "                     ha='center', va='center', rotation=30, transform=plt.gca().transAxes)\n",
    "            plt.savefig(f\"{cb}_{variable}_graph.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{ccy}_{variable}_graph.png\", bbox_inches='tight')\n",
    "            plt.close()\n",
    "            plt.close('all')\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            del g1\n",
    "            del df1\n",
    "            del fig\n",
    "            del ax\n",
    "            del ax2\n",
    "            \n",
    "    del list_ois\n",
    "    \n",
    "    #### graph time series\n",
    "    list_ois=[\"ois_1_0\",\"ois_2_0\",\"ois_3_0\",\"ois_4_0\",\"ois_5_0\",\"ois_6_0\",\"ois_7_0\",\"ois_8_0\",\n",
    "    \"ois_1_0\",\"ois_2_1\",\"ois_3_2\",\"ois_4_3\",\"ois_5_4\",\"ois_6_5\",\"ois_7_6\",\"ois_8_7\",\n",
    "    \"ois_2_0\",\"ois_3_1\",\"ois_4_2\",\"ois_5_3\",\"ois_6_4\",\"ois_7_5\",\"ois_8_6\",\n",
    "     \"ois_3_0\",\"ois_4_1\",\"ois_5_2\",\"ois_6_3\",\"ois_7_4\",\"ois_8_5\",\n",
    "     \"ois_4_0\",\"ois_5_1\",\"ois_6_2\",\"ois_7_3\",\"ois_8_4\",\n",
    "     \"ois_1_2_3\",\"ois_2_3_4\",\"ois_3_4_5\",\"ois_4_5_6\",\"ois_5_6_7\",\"ois_6_7_8\"]\n",
    "    \n",
    "    # Perform rolling and lag calculations\n",
    "    for variable in list_ois:\n",
    "        #print(f\"Processing variable: {variable}\")\n",
    "        if variable in df.columns:\n",
    "            \n",
    "            title_r=f\"{cb}_{variable}\"\n",
    "            y_label=\"Index %\"\n",
    "            x_label=title_r\n",
    "            df1=df[variable]\n",
    "            df1=df1.tail(72)\n",
    "            g1=df1.copy()\n",
    "            fig, ax = plt.subplots(figsize=(parameter1_s, \n",
    "                                            parameter2_s))\n",
    "            g1.plot(kind='line', ax=ax,\n",
    "                    y=[variable], color=\"blue\", \n",
    "                    linewidth=3, linestyle='-',\n",
    "                    marker='P', markersize=1)\n",
    "        \n",
    "            plt.axhline(0, color='black', lw=1, linestyle='dashed', alpha=0.75)\n",
    "            plt.style.use('default')\n",
    "            plt.ylabel(str(y_label), size=6)\n",
    "            plt.xlabel(str(x_label), size=6)\n",
    "            plt.tick_params(labelsize=2)\n",
    "            title=f\"{cb} {variable} \\n updt: {timestamp_date}\"\n",
    "            plt.title(str(title_r), size=7)\n",
    "            plt.grid(axis='both', alpha=.3)\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.set_ylim(ax.get_ylim())\n",
    "            plt.text(0.15, 0.15, str(variable), fontsize=40, color='gray', alpha=0.5,\n",
    "                     ha='center', va='center', rotation=30, transform=plt.gca().transAxes)\n",
    "            plt.savefig(f\"{cb}_{variable}_graph.png\", bbox_inches='tight')\n",
    "            plt.savefig(f\"{ccy}_{variable}_graph.png\", bbox_inches='tight')\n",
    "            plt.close()\n",
    "            plt.close('all')\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            del g1\n",
    "            del df1\n",
    "            del fig\n",
    "            del ax\n",
    "            del ax2\n",
    "\n",
    "    \n",
    "            \n",
    "    #### graph historgram #1 and #2 \n",
    "    # Loop through each column in the DataFrame\n",
    "    for variable in list_ois:\n",
    "        print(f\"Processing variable: {variable}\")\n",
    "        if variable in df.columns:\n",
    "            \n",
    "            #### graph historgram #1\n",
    "            # Extract the data for the current column\n",
    "            column_data = df[variable] * 100\n",
    "            column_data = column_data.tail(66)\n",
    "\n",
    "            # Skip if the column contains only NaNs\n",
    "            if column_data.isna().all():\n",
    "                print(f\"Skipping {variable} as it contains only NaN values.\")\n",
    "                continue\n",
    "\n",
    "            # Get the last value of the column\n",
    "            last_value = column_data.iloc[-1]\n",
    "    \n",
    "            # Create the histogram for the last 3 months\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(column_data.dropna(), bins=30, alpha=0.7, color=\"blue\", edgecolor=\"black\")\n",
    "            plt.axvline(last_value, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Last Value: {last_value:.2f}\")\n",
    "            title=f\"{text} histogram  - Distribution of {cb} {variable} last 3m \\n updt: {timestamp_date}\"\n",
    "            plt.title(title, fontsize=14)\n",
    "            plt.xlabel(\"Values\", fontsize=12)\n",
    "            plt.ylabel(\"Frequency\", fontsize=12)\n",
    "            plt.text(0.15, 0.15, str(variable), fontsize=40, color='gray', alpha=0.5,\n",
    "                     ha='center', va='center', rotation=30, transform=plt.gca().transAxes)\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "    \n",
    "            # Save the histogram image\n",
    "            plt.savefig(f\"{cb}_{variable}_histogram_3m.png\")\n",
    "            plt.savefig(f\"{ccy}_{variable}_histogram_3m.png\")\n",
    "            plt.close()\n",
    "\n",
    "        \n",
    "            #### graph historgram #2\n",
    "            # Create the histogram for the last 6 months\n",
    "            column_data = df[variable] * 100\n",
    "            column_data = column_data.tail(132)\n",
    "        \n",
    "            if column_data.isna().all():\n",
    "                print(f\"Skipping {variable} for 6m as it contains only NaN values.\")\n",
    "                continue\n",
    "        \n",
    "            last_value = column_data.iloc[-1]\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(column_data.dropna(), bins=30, alpha=0.7, color=\"green\", edgecolor=\"black\")\n",
    "            plt.axvline(last_value, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Last Value: {last_value:.2f}\")\n",
    "            title=f\"{text} histogram  - Distribution of {cb} {variable} last 6m \\n updt: {timestamp_date}\"\n",
    "            plt.title(title, fontsize=14)\n",
    "            plt.xlabel(\"Values\", fontsize=12)\n",
    "            plt.ylabel(\"Frequency\", fontsize=12)\n",
    "            plt.text(0.15, 0.15, str(variable), fontsize=40, color='gray', alpha=0.5,\n",
    "                     ha='center', va='center', rotation=30, transform=plt.gca().transAxes)\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "    \n",
    "            # Save the histogram image\n",
    "            plt.savefig(f\"{cb}_{variable}_histogram_6m.png\")\n",
    "            plt.savefig(f\"{ccy}_{variable}_histogram_6m.png\")\n",
    "            plt.close()\n",
    "\n",
    "    \n",
    "    print(f\"Completed analysis for {df_selected}. Results saved as {df_analysis}.\")\n",
    "\n",
    "\n",
    "#############\n",
    "# Ensure all indices are aligned\n",
    "df_analysis_agg = df_analysis_agg.sort_index()\n",
    "df_analysis_agg.to_parquet(\"df_analysis_agg.parquet\")\n",
    "\n",
    "# Ensure all indices are aligned\n",
    "df_analysis_agg = df_analysis_agg.sort_index()\n",
    "df_analysis_agg.to_parquet(\"df_analysis_agg.parquet\")\n",
    "print(\"Complete\")\n",
    "\n",
    "df_analysis_agg.to_clipboard()\n",
    "\n",
    "print(\"Complete\")\n",
    "\n",
    "### 004 analsys table  iamge\n",
    "\n",
    "#12-11 12-12, 12-13, (this group shoudl be white for example)\n",
    "#12-16,12-17,12-18,12-19,12-20 (this group shoudl be light gray  for example)\n",
    "#12-23,12-24,12-25,12-26,12-27 (this group shoudl be white for example)\n",
    "\n",
    "   # Perform calculations, ensuring to handle NaN\n",
    "from pandas.plotting import table\n",
    "cb_meeting_data=[\n",
    "    (\"usd\",\"fed_analysis\",\"usd - OIS HISTORICAL\"),\n",
    "    (\"cad\",\"boc_analysis\",\"CAD - OIS HISTORICAL\"),\n",
    "    (\"eur\",\"ecb_analysis\",\"EUR - OIS HISTORICAL\"),\n",
    "    (\"gbp\",\"boe_analysis\",\"GBP - OIS HISTORICAL\"),\n",
    "    (\"aud\",\"rba_analysis\",\"AUD - OIS HISTORICAL\"),\n",
    "    (\"nzd\",\"rbnz_analysis\",\"NZD - OIS HISTORICAL\"),\n",
    "    (\"nok\",\"norges_analysis\",\"NOK - OIS HISTORICAL\"),\n",
    "    (\"sek\",\"riksbank_analysis\",\"SEK- OIS HISTORICAL\"),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Columns to filter\n",
    "columns_to_calculate = [\"ois_1_0\",\"ois_2_0\",\"ois_3_0\",\"ois_4_0\",\"ois_5_0\",\"ois_6_0\",\"ois_7_0\",\"ois_8_0\",\" \",\n",
    "        \"ois_1_0\",\"ois_2_1\",\"ois_3_2\",\"ois_4_3\",\"ois_5_4\",\"ois_6_5\",\"ois_7_6\",\"ois_8_7\",\" \",\n",
    "        \"ois_2_0\",\"ois_3_1\",\"ois_4_2\",\"ois_5_3\",\"ois_6_4\",\"ois_7_5\",\"ois_8_6\",\" \",\n",
    "        \"ois_3_0\",\"ois_4_1\",\"ois_5_2\",\"ois_6_3\",\"ois_7_4\",\"ois_8_5\",\" \",\n",
    "        \"ois_4_0\",\"ois_5_1\",\"ois_6_2\",\"ois_7_3\",\"ois_8_4\",\" \",\n",
    "        \"ois_1_2_3\",\"ois_2_3_4\",\"ois_3_4_5\",\"ois_4_5_6\",\"ois_5_6_7\",\"ois_6_7_8\"]\n",
    "\n",
    "# Process each dataframe\n",
    "for ccy, bank, title in cb_meeting_data:\n",
    "    try:\n",
    "        print(f\"Processing dataframe: {bank}\")\n",
    "        \n",
    "        # Retrieve the dataframe\n",
    "        cb = globals()[bank]\n",
    "        cb=cb.tail(30)\n",
    "        cb=cb*100\n",
    "        cb=cb.round(1)\n",
    "        cb[\" \"] = None\n",
    "        # Filter for specified columns\n",
    "        selected_columns = [col for col in columns_to_calculate if col in cb.columns]\n",
    "        if not selected_columns:\n",
    "            print(f\"No matching columns found for {bank}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Selected columns: {selected_columns}\")\n",
    "        filtered_df = cb[selected_columns]\n",
    "        filtered_df.index = filtered_df.index.strftime('%m-%d')\n",
    "        # Add a blank column\n",
    "        #filtered_df[\" \"] = None\n",
    "        # Transpose the dataframe\n",
    "        transposed_df = filtered_df.T\n",
    "        transposed_df[\"index_\"]=transposed_df.index\n",
    "\n",
    "        # Get the week numbers for column headers\n",
    "        column_dates = pd.to_datetime(filtered_df.index, format='%m-%d')\n",
    "        week_numbers = column_dates.isocalendar().week.tolist()\n",
    "\n",
    "        # Assign alternating colors based on weeks\n",
    "        colors = []\n",
    "        current_week = None\n",
    "        for week in week_numbers:\n",
    "            if week != current_week:\n",
    "                current_week = week\n",
    "                current_color = 'lightgrey' if (len(colors) % 2 == 0) else 'white'\n",
    "            colors.append(current_color)\n",
    "        \n",
    "        # Create an image of the dataframe\n",
    "        fig, ax = plt.subplots(figsize=(20, len(transposed_df) * 0.3))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Create a table plot\n",
    "        table_obj = table(ax, transposed_df, loc=\"center\", cellLoc=\"center\")\n",
    "        table_obj.auto_set_font_size(False)\n",
    "        table_obj.set_fontsize(10)\n",
    "        table_obj.scale(1.2, 1.2)\n",
    "\n",
    "\n",
    "                    \n",
    "        # Add a title\n",
    "        plt.title(title, fontsize=14, pad=20)\n",
    "\n",
    "        # Save the image\n",
    "        image_path = f\"{ccy}_ois_historical.png\"\n",
    "        print(f\"Saving image to: {image_path}\")\n",
    "        plt.savefig(image_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"Saved image for {bank} successfully.\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Dataframe {bank} not found. Skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {bank}: {e}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 006 hisotirical curve CAHNGE\n",
    "\n",
    "#to see changes by meetings. \n",
    "#i wnat to run a loop that creates an image. \n",
    "#so i ahve a several dataframes  daily time series (10 rows)  in teh loop table . \n",
    "#the have following columns names\n",
    "# \"effective\",\"ois_0\",\"ois_1\",\"ois_2\",\"ois_3\",\"ois_4\",\"ois_5\",\"ois_6\",\"ois_7\",\"ois_8\"\n",
    "#sometimes tey only have until ois_4, some time they only have until ois_7.\n",
    "#i want to build a graph, in which for each line there is a plot line.\n",
    "#the most recent plot line is blue with withd line and markers and inteh last data points there is an arrow with annotation with he index values ( datetime)\n",
    "#the 4 previous row vales are with widhe dashed line and the inteh last data points there is an arrow with annotation with he index values ( datetime)\n",
    "#and int he 5 first row values are plot  as a small widht , thin dashed plot line \n",
    "#I don't want to consider the \"effective\" column for the graph.\n",
    "#i want the x axis  of row value column \"ois_0\" be o\n",
    "#i want the x axis  of row value column \"ois_1\" be 1\n",
    "# want the x axis  of row value column \"ois_2\" be 2\n",
    "#etc. \n",
    "#also add a legend. and a watermark that says in the bottom corner right \"is\"\n",
    "#take this as starting point \n",
    "\n",
    "meeting_data = [\n",
    "    (\"usd\",\"fed,\",\"fed_selected\",\"fed_analysis\",\"USD OIS HISTORICAL\"),\n",
    "    (\"cad\",\"boc\",\"boc_selected\",\"boc_analysis\",\"CAD OIS HISTORICAL\"),\n",
    "    (\"eur\",\"ecb\",\"ecb_selected\",\"ecb_analysis\",\"EUR OIS HISTORICAL\"),\n",
    "    (\"gbp\",\"boe\",\"boe_selected\",\"boe_analysis\",\"GBP OIS HISTORICAL\"),\n",
    "    (\"aud\",\"rba\",\"rba_selected\",\"rba_analysis\",\"AUD OIS HISTORICAL\"),\n",
    "    (\"nzd\",\"rbnz\",\"rbnz_selected\",\"rbnz_analysis\",\"NZD OIS HISTORICAL\"),\n",
    "    (\"nok\",\"norges\",\"norges_selected\",\"norges_analysis\",\"NOK OIS HISTORICAL\"),\n",
    "    (\"sek\",\"riksbank\",\"riksbank_selected\",\"riksbank_analysis\",\"SEK OIS HISTORICAL\"),\n",
    "]\n",
    "# \"effective\",\"ois_0\",\"ois_1\",\"ois_2\",\"ois_3\",\"ois_4\",\"ois_5\",\"ois_6\",\"ois_7\",\"ois_8\"\n",
    "\n",
    "# Process each pair of meetings and history dataframes\n",
    "for ccy,cb, df_selected, df_analysis, title in meeting_data:\n",
    "    print(f\"Processing dataframe: {df_selected}\")\n",
    "\n",
    "    # Retrieve the dataframe\n",
    "    try:\n",
    "        df_selected0 = globals()[df_selected]\n",
    "        print(f\"Loaded dataframe: {df_selected}\")\n",
    "    except KeyError:\n",
    "        print(f\"Dataframe {df_selected} not found. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    df = df_selected0.copy()\n",
    "\n",
    "    # Filter to the last 10 rows\n",
    "    print(f\"Filtering the dataframe to the last 10 rows...\")\n",
    "    df = df.tail(10)\n",
    "\n",
    "    # Exclude the \"effective\" column\n",
    "    plot_columns = [col for col in df.columns if \"ois_\" in col]\n",
    "    print(f\"Columns to be plotted: {plot_columns}\")\n",
    "\n",
    "    # Create x-axis mapping\n",
    "    x_values = range(len(plot_columns))\n",
    "    print(f\"X-axis values created: {list(x_values)}\")\n",
    "\n",
    "    # Initialize the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot each row of the dataframe\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"Plotting row with index: {idx}\")\n",
    "        row_values = row[plot_columns].values\n",
    "        label = idx.strftime('%Y-%m-%d')  # Use the index as the label\n",
    "\n",
    "        if idx == df.index[-1]:  # Most recent row\n",
    "            print(f\"Plotting the most recent row: {label}\")\n",
    "            plt.plot(\n",
    "                x_values, row_values, label=f\"{label} (Recent)\",\n",
    "                linewidth=2.5, color='blue', marker='o'\n",
    "            )\n",
    "            plt.annotate(\n",
    "                label,\n",
    "                xy=(x_values[-1], row_values[-1]),\n",
    "                xytext=(x_values[-1] + 0.1, row_values[-1]),\n",
    "                arrowprops=dict(facecolor='blue', arrowstyle=\"->\"),\n",
    "                fontsize=10, color='blue'\n",
    "            )\n",
    "        elif idx >= df.index[-5]:  # Previous 4 rows\n",
    "            print(f\"Plotting one of the 4 previous rows: {label}\")\n",
    "            plt.plot(\n",
    "                x_values, row_values, label=label,\n",
    "                linewidth=1.5, linestyle='--', color='black'\n",
    "            )\n",
    "            plt.annotate(\n",
    "                label,\n",
    "                xy=(x_values[-1], row_values[-1]),\n",
    "                xytext=(x_values[-1] + 0.1, row_values[-1]),\n",
    "                arrowprops=dict(facecolor='blue', arrowstyle=\"->\"),\n",
    "                fontsize=10, color='blue'\n",
    "            )\n",
    "        else:  # Remaining older rows\n",
    "            print(f\"Plotting older row: {label}\")\n",
    "            plt.plot(\n",
    "                x_values, row_values, label=label,\n",
    "                linewidth=0.8, linestyle='--', color='black'\n",
    "            )\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xticks(x_values, [col.replace(\"ois_\", \"\") for col in plot_columns])\n",
    "    plt.xlabel(\"OIS Terms\", fontsize=12)\n",
    "    plt.ylabel(\"Values\", fontsize=12)\n",
    "    plt.title(f\"{title} - Historical OIS Plot\", fontsize=14)\n",
    "    plt.legend(\n",
    "    loc='upper center',  # Position at the top center\n",
    "    bbox_to_anchor=(0.5, 0.95),  # Slightly below the top of the plot area\n",
    "    ncol=3,  # Number of columns for the legend items\n",
    "    fontsize=10)\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Add watermark\n",
    "    plt.text(\n",
    "        0.1, 0.01, \"OIS meetings\", fontsize=20, color='gray',\n",
    "        ha='right', va='bottom', alpha=0.5, transform=plt.gca().transAxes\n",
    "    )\n",
    "\n",
    "    # Save the plot\n",
    "    print(f\"Saving plot to: {image_path}\")\n",
    "    plt.savefig(f\"{df_analysis}_ois_historical.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(f\"{ccy}_ois_historical.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Completed processing for: {df_selected}\")\n",
    "\n",
    "\n",
    "### 007 Correlation with USD FED MEETINGS\n",
    "\n",
    "#correlations each meeting swith \n",
    "#Grab dataframe create correlation ehatmpa save images.  30 dasy s, 60 days 90 days\n",
    "\n",
    "#i wnat to run a loop that creates an image of correlation heatmpas for 30 days, 60 days and 90 days. \n",
    "#so i ahve a several dataframes  daily time series  in teh loop table .\n",
    "#i ahve them for the following list of banks = (fed, ecb,boe, boc, rba, rbnz, norges, riksbank)\n",
    "#the have following columns names\n",
    "# \"effective\",\"ois_0\",\"ois_1\",\"ois_2\",\"ois_3\",\"ois_4\",\"ois_5\",\"ois_6\",\"ois_7\",\"ois_8\"\n",
    "#sometimes tey only have until ois_4, some time they only have until ois_7.\n",
    "#i want to build a correlation heatmap betweeen the meetings from teh FED dataframe adn teh rest of central banks. \n",
    "#in case there is a dataframe that doenst have some columns likes \"ois_7\" jsut put nans and keep going. \n",
    "#each correlation heatmap has to have labels, titel and be saved as png image. \n",
    "#take this as starting point \n",
    "\n",
    "meeting_data = [\n",
    "    (\"fed_selected\",\"USD OIS HISTORICAL\"),\n",
    "    (\"boc_selected\",\"CAD OIS HISTORICAL\"),\n",
    "    (\"ecb_selected\",\"EUR OIS HISTORICAL\"),\n",
    "    (\"boe_selected\",\"GBP OIS HISTORICAL\"),\n",
    "    (\"rba_selected\",\"AUD OIS HISTORICAL\"),\n",
    "    (\"rbnz_selected\",\"NZD OIS HISTORICAL\"),\n",
    "    (\"norges_selected\",\"NOK OIS HISTORICAL\"),\n",
    "    (\"riksbank_selected\",\"SEK OIS HISTORICAL\"),\n",
    "]\n",
    "# \"effective\",\"ois_0\",\"ois_1\",\"ois_2\",\"ois_3\",\"ois_4\",\"ois_5\",\"ois_6\",\"ois_7\",\"ois_8\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# List of banks\n",
    "banks = [\"fed_selected\", \"ecb_selected\", \n",
    "         \"boe_selected\", \"boc_selected\", \n",
    "         \"rba_selected\", \"rbnz_selected\", \"norges_selected\", \"riksbank_selected\"]\n",
    "\n",
    "# Process data for correlation heatmaps\n",
    "for bank in banks:\n",
    "    try:\n",
    "        # Retrieve the FED dataframe as reference\n",
    "        fed_df = globals()[\"fed_selected\"]\n",
    "        fed_df = fed_df[[\"ois_0\", \"ois_1\", \"ois_2\", \"ois_3\", \"ois_4\", \"ois_5\", \"ois_6\", \"ois_7\", \"ois_8\"]].tail(90)\n",
    "\n",
    "        # Retrieve the current bank dataframe\n",
    "        bank_df = globals()[bank]\n",
    "        bank_df = bank_df[[\"ois_0\", \"ois_1\", \"ois_2\", \"ois_3\", \"ois_4\", \"ois_5\", \"ois_6\", \"ois_7\", \"ois_8\"]].tail(90)\n",
    "\n",
    "        # Ensure all required columns exist in both dataframes, filling missing columns with NaN\n",
    "        required_columns = [f\"ois_{i}\" for i in range(9)]\n",
    "        for col in required_columns:\n",
    "            if col not in fed_df.columns:\n",
    "                fed_df[col] = np.nan\n",
    "            if col not in bank_df.columns:\n",
    "                bank_df[col] = np.nan\n",
    "                \n",
    "\n",
    "        # Calculate correlations for the past 30, 60, and 90 days\n",
    "        for window, days in zip([30, 60, 90], [\"30d\", \"60d\", \"90d\"]):\n",
    "            print(f\"Processing {bank} correlation heatmap for {days}...\")\n",
    "            fed_subset = fed_df.tail(window)\n",
    "            bank_subset = bank_df.tail(window)\n",
    "\n",
    "            # Combine both dataframes for correlation\n",
    "            combined_df = pd.concat([fed_subset, bank_subset], axis=1, keys=[\"FED\", bank.upper()])\n",
    "            correlation_matrix = combined_df.corr()\n",
    "\n",
    "            # Create a heatmap\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(\n",
    "                correlation_matrix,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cmap=\"coolwarm\",\n",
    "                cbar=True,\n",
    "                square=True,\n",
    "                linewidths=0.5\n",
    "            )\n",
    "            plt.title(f\"Correlation Heatmap: FED vs {bank.upper()} ({days})\", fontsize=14)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.yticks(rotation=0)\n",
    "\n",
    "            # Save the heatmap image\n",
    "            image_path = f\"complete_correlation_heatmap_fed_vs_{bank}_{days}.png\"\n",
    "            plt.savefig(image_path, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            print(f\"Saved heatmap: {image_path}\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Dataframe for {bank} not found. Skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {bank}: {e}\")\n",
    "\n",
    "meeting_data = [\n",
    "    (\"fed_selected\",\"USD OIS HISTORICAL\"),\n",
    "    (\"boc_selected\",\"CAD OIS HISTORICAL\"),\n",
    "    (\"ecb_selected\",\"EUR OIS HISTORICAL\"),\n",
    "    (\"boe_selected\",\"GBP OIS HISTORICAL\"),\n",
    "    (\"rba_selected\",\"AUD OIS HISTORICAL\"),\n",
    "    (\"rbnz_selected\",\"NZD OIS HISTORICAL\"),\n",
    "    (\"norges_selected\",\"NOK OIS HISTORICAL\"),\n",
    "    (\"riksbank_selected\",\"SEK OIS HISTORICAL\"),\n",
    "]\n",
    "# \"effective\",\"ois_0\",\"ois_1\",\"ois_2\",\"ois_3\",\"ois_4\",\"ois_5\",\"ois_6\",\"ois_7\",\"ois_8\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# List of banks\n",
    "banks = [\"fed_selected\", \"ecb_selected\", \n",
    "         \"boe_selected\", \"boc_selected\", \n",
    "         \"rba_selected\", \"rbnz_selected\", \"norges_selected\", \"riksbank_selected\"]\n",
    "\n",
    "# Process data for correlation heatmaps\n",
    "for bank in banks:\n",
    "    try:\n",
    "        # Retrieve the FED dataframe as reference\n",
    "        fed_df = globals()[\"fed_selected\"]\n",
    "        fed_df = fed_df[[\"ois_0\", \"ois_1\", \"ois_2\", \"ois_3\", \"ois_4\", \"ois_5\", \"ois_6\", \"ois_7\", \"ois_8\"]].tail(90)\n",
    "\n",
    "        # Retrieve the current bank dataframe\n",
    "        bank_df = globals()[bank]\n",
    "        bank_df = bank_df[[\"ois_0\", \"ois_1\", \"ois_2\", \"ois_3\", \"ois_4\", \"ois_5\", \"ois_6\", \"ois_7\", \"ois_8\"]].tail(90)\n",
    "\n",
    "        # Ensure all required columns exist in both dataframes, filling missing columns with NaN\n",
    "        required_columns = [f\"ois_{i}\" for i in range(9)]\n",
    "        for col in required_columns:\n",
    "            if col not in fed_df.columns:\n",
    "                fed_df[col] = np.nan\n",
    "            if col not in bank_df.columns:\n",
    "                bank_df[col] = np.nan\n",
    "\n",
    "       # Calculate correlations for the past 30, 60, and 90 days\n",
    "        for window, days in zip([30, 60, 90], [\"30d\", \"60d\", \"90d\"]):\n",
    "            print(f\"Processing {bank} correlation heatmap for {days}...\")\n",
    "            fed_subset = fed_df.tail(window)\n",
    "            bank_subset = bank_df.tail(window)\n",
    "\n",
    "            # Combine both dataframes for correlation\n",
    "            combined_df = pd.concat([fed_subset, bank_subset], axis=1, keys=[\"FED\", bank.upper()])\n",
    "            correlation_matrix = combined_df.corr()\n",
    "\n",
    "            # Extract only the quadrant of interest\n",
    "            quadrant = correlation_matrix.loc[\n",
    "                (\"FED\", slice(None)),\n",
    "                (bank.upper(), slice(None))\n",
    "            ]\n",
    "\n",
    "            # Create a heatmap for the quadrant\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(\n",
    "                quadrant,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cmap=\"coolwarm\",\n",
    "                cbar=True,\n",
    "                square=True,\n",
    "                linewidths=0.5\n",
    "            )\n",
    "            plt.title(f\"Correlation Heatmap: FED vs {bank.upper()} ({days})\", fontsize=14)\n",
    "\n",
    "            # Set axis labels\n",
    "            plt.xlabel(f\"{bank.upper()} Meetings (Columns)\", fontsize=12)\n",
    "            plt.ylabel(\"FED Meetings (Rows)\", fontsize=12)\n",
    "\n",
    "            # Set y-axis tick labels as \"ois_*\" and invert the order\n",
    "            y_labels = quadrant.index.get_level_values(1)\n",
    "            plt.yticks(ticks=np.arange(len(y_labels)) + 0.5, labels=y_labels, rotation=0)\n",
    "            plt.gca().invert_yaxis()  # Flip the y-axis to start with \"ois_0\" at the bottom\n",
    "\n",
    "            # Set x-axis tick labels as \"ois_*\"\n",
    "            x_labels = quadrant.columns.get_level_values(1)\n",
    "            plt.xticks(ticks=np.arange(len(x_labels)) + 0.5, labels=x_labels, rotation=45)\n",
    "\n",
    "            # Save the heatmap image\n",
    "            image_path = f\"correlation_quadrant_fed_vs_{bank}_{days}.png\"\n",
    "            plt.savefig(image_path, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            print(f\"Saved heatmap: {image_path}\")\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Dataframe for {bank} not found. Skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {bank}: {e}\")\n",
    "\n",
    "### 008 boxes with USD FED OIS MEETINGS\n",
    "\n",
    "\n",
    "\n",
    "#i wnat to run a loop that creates an NEW DATAfrme taht is the resutls ofmaking a subtraction between a cenntra l bank tabler (loop dependent)\n",
    "#and the \"fed_analysis\" table. \n",
    "#the central bank table is part of a list. \n",
    "#In case one of the dataframa doenst posses one of the oclumn for the oepration thne move to the next one.\n",
    "#Supposedly whne making the oepration both dataframe shoudl ahve same index, but double check before doing\n",
    "#                                  take this as starting point \n",
    "\n",
    "\n",
    "\n",
    "fed_meeting_data = [\n",
    "(\"fed_analysis\",\"USD OIS HISTORICAL\"),]\n",
    "\n",
    "cb_meeting_data=[\n",
    "    (\"cad\",\"boc\",\"boc_analysis\",\"box_boc_analysis\",\"BOX CAD - USD OIS HISTORICAL\"),\n",
    "    (\"eur\",\"ecb\",\"ecb_analysis\",\"box_ecb_analysis\",\"BOX EUR - USD OIS HISTORICAL\"),\n",
    "    (\"gbp\",\"boe\",\"boe_analysis\",\"box_boe_analysis\",\"BOX GBP - USD OIS HISTORICAL\"),\n",
    "    (\"aud\",\"rba\",\"rba_analysis\",\"box_rba_analysis\",\"BOX AUD - USD OIS HISTORICAL\"),\n",
    "    (\"nzd\",\"rbnz\",\"rbnz_analysis\",\"box_rbnz_analysis\",\"BOX NZD - USD OIS HISTORICAL\"),\n",
    "    (\"nok\",\"norges\",\"norges_analysis\",\"box_norges_analysis\",\"BOX NOK - USD OIS HISTORICAL\"),\n",
    "    (\"sek\",\"riksbank\",\"riksbank_analysis\",\"box_riksbank_analysis\",\"BOX SEK- USD OIS HISTORICAL\"),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Columns to perform operations on\n",
    "columns_to_calculate = [\n",
    "    \"ois_1_0\", \"ois_2_0\", \"ois_3_0\", \"ois_4_0\", \"ois_5_0\", \"ois_6_0\", \"ois_7_0\", \"ois_8_0\",\n",
    "    \"ois_2_1\", \"ois_3_2\", \"ois_4_3\", \"ois_5_4\", \"ois_6_5\", \"ois_7_6\", \"ois_8_7\",\n",
    "    \"ois_3_1\", \"ois_4_2\", \"ois_5_3\", \"ois_6_4\", \"ois_7_5\", \"ois_8_6\",\n",
    "    \"ois_4_1\", \"ois_5_2\", \"ois_6_3\", \"ois_7_4\", \"ois_8_5\", \"ois_5_1\", \"ois_6_2\", \"ois_7_3\"\n",
    "]\n",
    "\n",
    "df_box_agg = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Process data for subtractions\n",
    "for ccy, cb, table_analysis, new_df_name, title in cb_meeting_data:\n",
    "    try:\n",
    "        # Retrieve the FED dataframe as reference\n",
    "        fed = globals()[\"fed_analysis\"]\n",
    "        # Retrieve the current bank dataframe\n",
    "        cb = globals()[table_analysis]\n",
    "        \n",
    "        # Ensure the indices are aligned\n",
    "        if not fed.index.equals(cb.index):\n",
    "            print(f\"Index mismatch between FED and {bank}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Create a new dataframe for the results\n",
    "        new_df = pd.DataFrame(index=fed.index)\n",
    "\n",
    "        # Perform subtractions for matching columns\n",
    "        for col in columns_to_calculate:\n",
    "            if col in cb.columns and col in fed.columns:\n",
    "                print(f\"Calculating {col} for {ccy}...\")\n",
    "                new_df[f\"box_{ccy}_usd_{col}\"] = cb[col] - fed[col]\n",
    "            else:\n",
    "                print(f\"Column {col} not found in {ccy} or FED. Skipping this column...\")\n",
    "\n",
    "        # Save the resulting dataframe globally\n",
    "        globals()[new_df_name] = new_df.copy()\n",
    "        print(f\"Created dataframe: {new_df_name}\")\n",
    "\n",
    "\n",
    "        # Add the prefix to column names\n",
    "        #df_agg.columns = [f\"{cb}_{col}\" if col != df.index.name else col for col in df_agg.columns]\n",
    "        # Concatenate the dataframe horizontally\n",
    "        df_box_agg = pd.concat([df_box_agg, new_df], axis=1)    \n",
    "\n",
    "\n",
    "    ########################################################################\n",
    "    #########################################################################\n",
    "\n",
    "\n",
    "\n",
    "        # Perform rolling and lag calculations\n",
    "        list_ois=new_df.columns.tolist()\n",
    "        for variable in list_ois:\n",
    "            #print(f\"Processing variable: {variable}\")\n",
    "            if variable in new_df.columns:\n",
    "                \n",
    "                title_r=f\"{variable}\"\n",
    "                y_label=\"Index %\"\n",
    "                x_label=title_r\n",
    "                df1=new_df[variable]\n",
    "                df1=df1.tail(72)\n",
    "                g1=df1.copy()\n",
    "                fig, ax = plt.subplots(figsize=(parameter1_s, \n",
    "                                                parameter2_s))\n",
    "                g1.plot(kind='line', ax=ax,\n",
    "                        y=[variable], color=\"blue\", \n",
    "                        linewidth=3, linestyle='-',\n",
    "                        marker='P', markersize=1)\n",
    "            \n",
    "                plt.axhline(0, color='black', lw=1, linestyle='dashed', alpha=0.75)\n",
    "                plt.style.use('default')\n",
    "                plt.ylabel(str(y_label), size=6)\n",
    "                plt.xlabel(str(x_label), size=6)\n",
    "                plt.tick_params(labelsize=2)\n",
    "                plt.title(str(variable), size=7)\n",
    "                plt.grid(axis='both', alpha=.3)\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.set_ylim(ax.get_ylim())\n",
    "                plt.text(0.15, 0.15, str(variable), fontsize=40, color='gray', alpha=0.5,\n",
    "                         ha='center', va='center', rotation=30, transform=plt.gca().transAxes)\n",
    "                plt.savefig(f\"{variable}_graph.png\", bbox_inches='tight')\n",
    "                plt.close()\n",
    "                plt.close('all')\n",
    "                plt.clf()\n",
    "                plt.cla()\n",
    "                del g1\n",
    "                del df1\n",
    "                del fig\n",
    "                del ax\n",
    "                del ax2\n",
    "            \n",
    "\n",
    "        # Loop through each column in the DataFrame\n",
    "        for variable in list_ois:\n",
    "            print(f\"Processing variable: {variable}\")\n",
    "            if variable in new_df.columns:\n",
    "                \n",
    "                #### graph historgram #1\n",
    "                # Extract the data for the current column\n",
    "                column_data = new_df[variable] * 100\n",
    "                column_data = column_data.tail(66)\n",
    "    \n",
    "                # Skip if the column contains only NaNs\n",
    "                if column_data.isna().all():\n",
    "                    print(f\"Skipping {variable} as it contains only NaN values.\")\n",
    "                    continue\n",
    "    \n",
    "                # Get the last value of the column\n",
    "                last_value = column_data.iloc[-1]\n",
    "        \n",
    "                # Create the histogram for the last 3 months\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.hist(column_data.dropna(), bins=30, alpha=0.7, color=\"blue\", edgecolor=\"black\")\n",
    "                plt.axvline(last_value, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Last Value: {last_value:.2f}\")\n",
    "                title=f\"{varaible}_histogram\"\n",
    "                plt.title(f\"{title} - Distribution of {cb} {variable} last 3m\", fontsize=14)\n",
    "                plt.xlabel(\"Values\", fontsize=12)\n",
    "                plt.ylabel(\"Frequency\", fontsize=12)\n",
    "                plt.legend()\n",
    "                plt.grid(alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "        \n",
    "                # Save the histogram image\n",
    "                image_path = f\"{variable}_histogram_3m.png\"\n",
    "                print(f\"Saving histogram: {image_path}\")\n",
    "                plt.savefig(image_path)\n",
    "                plt.close()\n",
    "    \n",
    "            \n",
    "                #### graph historgram #2\n",
    "                # Create the histogram for the last 6 months\n",
    "                column_data = new_df[variable] * 100\n",
    "                column_data = column_data.tail(132)\n",
    "            \n",
    "                if column_data.isna().all():\n",
    "                    print(f\"Skipping {variable} for 6m as it contains only NaN values.\")\n",
    "                    continue\n",
    "            \n",
    "                last_value = column_data.iloc[-1]\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.hist(column_data.dropna(), bins=30, alpha=0.7, color=\"green\", edgecolor=\"black\")\n",
    "                plt.axvline(last_value, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Last Value: {last_value:.2f}\")\n",
    "                title=f\"{variable}_histogram\"\n",
    "                plt.title(f\"{title} - Distribution of {cb} {variable} last 6m\", fontsize=14)\n",
    "                plt.xlabel(\"Values\", fontsize=12)\n",
    "                plt.ylabel(\"Frequency\", fontsize=12)\n",
    "                plt.legend()\n",
    "                plt.grid(alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "        \n",
    "                # Save the histogram image\n",
    "                image_path = f\"{variable}_histogram_6m.png\"\n",
    "                print(f\"Saving histogram: {image_path}\")\n",
    "                plt.savefig(image_path)\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    except KeyError:\n",
    "        print(f\"Dataframe {table_analysis} not found. Skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {bank}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "#############\n",
    "# Ensure all indices are aligned\n",
    "df_box_agg = df_box_agg.sort_index()\n",
    "df_box_agg.to_parquet(\"df_box_agg.parquet\")\n",
    "\n",
    "\n",
    "# Ensure all indices are aligned\n",
    "df_box_agg = df_box_agg.sort_index()\n",
    "df_box_agg.to_parquet(\"df_box_agg.parquet\")\n",
    "print(\"Compelte\")\n",
    "\n",
    "print(\"Compelte\")\n",
    "\n",
    "\n",
    "\n",
    "### -----------------------------------------------------------------------------------------------\n",
    "\n",
    "### changing dir \n",
    "\n",
    "os.chdir('C:/database/spreadsheet/')\n",
    "historical_ois = pd.read_parquet(\"central_bank_meetings_historical_pricing.parquet\")\n",
    "historical_ois.index=historical_ois.index.date\n",
    "print(historical_ois.shape[0])\n",
    "print(historical_ois.shape[1])\n",
    "print(historical_ois.index.min())\n",
    "print(historical_ois.index.max())\n",
    "\n",
    "#print(historical_ois.columns.tolist())\n",
    "\n",
    "list_fed_ois =  [ 'fed-2021-01', 'fed-2021-03', 'fed-2021-04', \n",
    "                 'fed-2021-06', 'fed-2021-07', 'fed-2021-09',\n",
    "                 'fed-2021-11', 'fed-2021-12', 'fed-2022-01', \n",
    "                 'fed-2022-03', 'fed-2022-05', 'fed-2022-06', \n",
    "                 'fed-2022-07', 'fed-2022-09', 'fed-2022-11', \n",
    "                 'fed-2022-12', 'fed-2023-02', 'fed-2023-03', \n",
    "                 'fed-2023-05', 'fed-2023-06', 'fed-2023-07', \n",
    "                 'fed-2023-09', 'fed-2023-11', 'fed-2023-12', \n",
    "                 'fed-2024-01', 'fed-2024-03', 'fed-2024-05',\n",
    "                 'fed-2024-06', 'fed-2024-07', 'fed-2024-09',\n",
    "                 'fed-2024-11', 'fed-2024-12', \n",
    "                 'fed-2025-01', 'fed-2025-03', 'fed-2025-05',\n",
    "                 'fed-2025-06', 'fed-2025-07', 'fed-2025-09', \n",
    "                 'fed-2025-10', 'fed-2025-12',]\n",
    "\n",
    "\n",
    "list_fed_ois =  [ \n",
    "                 'fed-2024-11', 'fed-2024-12', \n",
    "                 'fed-2025-01', 'fed-2025-03', 'fed-2025-05',\n",
    "                 'fed-2025-06', 'fed-2025-07', 'fed-2025-09', \n",
    "                 'fed-2025-10', 'fed-2025-12',]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### loading futures \n",
    "\n",
    "#load impulse reposne\n",
    "# do regressions specifcally fo ois meeting and specifc contracts. \n",
    "#find the beta\n",
    "#rolling beta regressions, compare by month \n",
    "#start impulse reposnse funciton reaction \n",
    "\n",
    "\n",
    "os.chdir('C:/arm 2024_requiem/hedge_fund/data/hf_spreadsheets')\n",
    "futures = pd.read_excel(\"arm_rates_futures_data.xlsx\", sheet_name=\"data\", index_col=0, parse_dates=True)\n",
    "futures.index=futures.index.date\n",
    "print(futures.shape[0])\n",
    "print(futures.shape[1])\n",
    "print(futures.index.min())\n",
    "print(futures.index.max())\n",
    "futures.columns = futures.columns.str.lower()\n",
    "\n",
    "#print(futures.columns.tolist())\n",
    "\n",
    "list_sfr = ['sfrm5u5 comdty', 'sfru5z5 comdty', 'sfrz5h6 comdty',\n",
    "            'sfrh6m6 comdty', 'sfrm6u6 comdty', 'sfru6z6 comdty', \n",
    "            'sfrz6h7 comdty', 'sfrh7m7 comdty', 'sfrm7u7 comdty', \n",
    "            'sfru7z7 comdty', 'sfrz7h8 comdty', 'sfrh8m8 comdty', \n",
    "            'sfrm8u8 comdty', 'sfru8z8 comdty', 'sfrh5u5 comdty', \n",
    "            'sfrm5z5 comdty', 'sfru5h6 comdty', 'sfrz5m6 comdty', \n",
    "            'sfrh6u6 comdty', 'sfrm6z6 comdty', 'sfru6h7 comdty',\n",
    "            'sfrz6m7 comdty', 'sfrh7u7 comdty', 'sfrm7z7 comdty', \n",
    "            'sfru7h8 comdty', 'sfrz7m8 comdty', 'sfrh8u8 comdty', \n",
    "            'sfrm8z8 comdty', 'sfrh5z5 comdty', 'sfrm5h6 comdty',\n",
    "            'sfru5m6 comdty',]\n",
    "\n",
    "\n",
    "### regresisons loop \n",
    "\n",
    "i have one dataframe (ois dataframe) that has daily time series with several columns. \n",
    "i have a seocnd dataframe (futures dataframe) that has daily time series. \n",
    "both datafram have diufferent column names.\n",
    "i wna to run a loop that generates the folling. \n",
    "i wnat ot run a loop taht goes all the combination of column from dataframe one and column from dataframe two. \n",
    "in each loop i want to run a regression between the values of (y is the futures dataframe columns and x is the ois dataframe column). but the regression only consider the vlaues from a specifc year and month. for example the regression  (without intercept ) of values from year 2023, month of june.  and thne would do the same regression but for the year 2023, month july data. \n",
    "in the end i want to save the  beta of the regressions in a matrix. so in the index i have the combiantion  of columns (x,y) nad first column has year 2023 , month july and the row vlaue has the specifc beta of that regression.  \n",
    "\n",
    "fut000=futures[list_sfr]\n",
    "ois000=historical_ois[list_fed_ois]\n",
    "print(ois000.columns.tolist())\n",
    "\n",
    "from itertools import product\n",
    "ois_data=ois000.copy()\n",
    "futures_data=fut000.copy()\n",
    "ois_data.index = pd.to_datetime(ois_data.index)\n",
    "futures_data.index = pd.to_datetime(futures_data.index)\n",
    "\n",
    "\n",
    "# Create an empty DataFrame to store beta values\n",
    "beta_matrix = pd.DataFrame()\n",
    "\n",
    "# Generate all combinations of columns\n",
    "combinations = list(product(ois_data.columns, futures_data.columns))\n",
    "print(combinations)\n",
    "\n",
    "\n",
    "# Loop through each combination\n",
    "for ois_col, futures_col in combinations:\n",
    "    for year in range(2024, 2025):  # Define your year range\n",
    "        for month in range(1, 13):  # Loop through all months\n",
    "\n",
    "            # Filter data by year and month\n",
    "            ois_filtered = ois_data.loc[(ois_data.index.year == year) & (ois_data.index.month == month), ois_col]\n",
    "            futures_filtered = futures_data.loc[(futures_data.index.year == year) & (futures_data.index.month == month), futures_col]\n",
    "\n",
    "            # Align data by index to ensure consistent date alignment\n",
    "            merged_data = pd.concat([ois_filtered, futures_filtered], axis=1).dropna()\n",
    "\n",
    "            # Perform regression only if sufficient data exists\n",
    "            if len(merged_data) > 5:\n",
    "                X = merged_data[ois_col].values\n",
    "                y = merged_data[futures_col].values\n",
    "\n",
    "                # Regression without intercept\n",
    "                beta = np.linalg.lstsq(X.reshape(-1, 1), y, rcond=None)[0][0]\n",
    "\n",
    "                # Ensure index exists before assigning\n",
    "                if (ois_col, futures_col) not in beta_matrix.index:\n",
    "                    beta_matrix = beta_matrix.reindex(beta_matrix.index.union([(ois_col, futures_col)]))\n",
    "\n",
    "                # Assign the beta value\n",
    "                beta_matrix.loc[(ois_col, futures_col), f'{year}-{month:02d}'] = beta\n",
    "\n",
    "print(beta_matrix)\n",
    "beta_matrix.to_clipboard()\n",
    "\n",
    "#print(merged_data)\n",
    "\n",
    "#print(ois_filtered)\n",
    "\n",
    "#print(futures_filtered)\n",
    "\n",
    "### impulse reaciton fucntion grapsh\n",
    "\n",
    "\n",
    "\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "\n",
    "# Create a dictionary to store IRF results\n",
    "irf_results = {}\n",
    "\n",
    "# Generate all combinations of columns\n",
    "combinations = list(product(ois_data.columns, futures_data.columns))\n",
    "n=0\n",
    "# Loop through each combination\n",
    "for ois_col, futures_col in combinations:\n",
    "\n",
    "    # Filter for the last 90 days\n",
    "    start_date = ois_data.index[-90]  # 90-day lookback\n",
    "    ois_filtered = ois_data.loc[start_date:, ois_col]\n",
    "    futures_filtered = futures_data.loc[start_date:, futures_col]\n",
    "\n",
    "    # Align data by index to ensure consistent date alignment\n",
    "    merged_data = pd.concat([ois_filtered, futures_filtered], axis=1).dropna()\n",
    "\n",
    "    # Perform VAR model only if sufficient data exists\n",
    "    if len(merged_data) > 30:  # Ensure enough observations\n",
    "        # Fit VAR model\n",
    "        model = VAR(merged_data)\n",
    "        model_fitted = model.fit(maxlags=5)\n",
    "\n",
    "        # Impulse Response Function (IRF)\n",
    "        irf = model_fitted.irf(10)  # 10-period response\n",
    "\n",
    "        # Store IRF values\n",
    "        irf_values = irf.irfs[:, 0, 1]  # Impact of OIS on Futures\n",
    "        irf_results[(ois_col, futures_col)] = irf_values\n",
    "\n",
    "        # Visualize IRF\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(irf_values, marker='o', label=f'{ois_col}  {futures_col}')\n",
    "        plt.axhline(0, color='gray', linestyle='--')\n",
    "        plt.title(f'Impulse Response: {ois_col}  {futures_col}')\n",
    "        plt.xlabel('Days')\n",
    "        plt.ylabel('Impact')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"ir_{n}.png\")\n",
    "        plt.show()\n",
    "\n",
    "    n=n+1\n",
    "\n",
    "# Convert IRF results to DataFrame\n",
    "irf_df = pd.DataFrame.from_dict(irf_results, orient='index')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### impulse reachtions  forecasts  level  */ spread\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Dictionary to store individual forecasts for aggregation\n",
    "forecast_dict = defaultdict(list)\n",
    "\n",
    "# Robustness Checks\n",
    "def check_stationarity(series, threshold=0.05):\n",
    "    \"\"\"Perform ADF test to check stationarity.\"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    return result[1] < threshold\n",
    "\n",
    "def enforce_stationarity(series):\n",
    "    \"\"\"Enforce stationarity by differencing if needed.\"\"\"\n",
    "    return series.diff().dropna() if not check_stationarity(series) else series\n",
    "\n",
    "def remove_outliers(series, threshold=3.0):\n",
    "    \"\"\"Removes extreme outliers using z-score trimming.\"\"\"\n",
    "    z_scores = (series - series.mean()) / series.std()\n",
    "    return series[(z_scores > -threshold) & (z_scores < threshold)]\n",
    "\n",
    "def validate_data(df, min_length=30):\n",
    "    \"\"\"Check data consistency for stability.\"\"\"\n",
    "    if len(df) < min_length or df.isnull().sum().sum() > 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Generate combinations of columns\n",
    "combinations = list(product(ois_data.columns, futures_data.columns))\n",
    "print(combinations)\n",
    "\n",
    "# Step 1: Individual Forecasts\n",
    "for ois_col, futures_col in combinations:\n",
    "\n",
    "    # Filter for the last 90 days\n",
    "    start_date = ois_data.index[-90]\n",
    "    ois_filtered = ois_data.loc[start_date:, ois_col]\n",
    "    futures_filtered = futures_data.loc[start_date:, futures_col]\n",
    "\n",
    "    # Align data by index\n",
    "    merged_data = pd.concat([ois_filtered, futures_filtered], axis=1).dropna()\n",
    "\n",
    "    # Consistency Check\n",
    "    if not validate_data(merged_data):\n",
    "        continue\n",
    "\n",
    "    # Ensure stationarity & remove outliers\n",
    "    #merged_data[ois_col] = remove_outliers(enforce_stationarity(merged_data[ois_col]))\n",
    "    #merged_data[futures_col] = remove_outliers(enforce_stationarity(merged_data[futures_col]))\n",
    "\n",
    "    # Fit VAR model (Adaptive Lags)\n",
    "    model = VAR(merged_data)\n",
    "    model_fitted = model.fit(maxlags=min(3, len(merged_data) // 4))\n",
    "\n",
    "    # Forecasting\n",
    "    forecast_steps = 5\n",
    "    forecast = model_fitted.forecast(merged_data.values[-5:], steps=forecast_steps)\n",
    "\n",
    "    # Store individual forecasts\n",
    "    forecast_index = pd.date_range(start=ois_data.index[-1] + pd.Timedelta(days=1), periods=forecast_steps)\n",
    "    forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=merged_data.columns)\n",
    "\n",
    "    # Collect forecasts in the dictionary for aggregation\n",
    "    forecast_dict[futures_col].append(forecast_df[futures_col])\n",
    "\n",
    "    # Visualize Individual Forecast\n",
    "    plt.plot(merged_data[futures_col].iloc[-30:], label='Actual', color='blue')\n",
    "    plt.plot(forecast_df[futures_col], label=f'Forecast {ois_col}  {futures_col}', linestyle='--')\n",
    "    plt.axvline(x=forecast_index[0], color='gray', linestyle='--')\n",
    "    plt.title(f'Forecast: {ois_col}  {futures_col}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#print(merged_data)\n",
    "#merged_data=merged_data.dropna()\n",
    "\n",
    "# Step 2: Aggregated Forecasts\n",
    "aggregated_forecasts = pd.DataFrame()\n",
    "\n",
    "for futures_col, forecasts in forecast_dict.items():\n",
    "    print(futures_col, forecasts)\n",
    "    # Mean Forecast\n",
    "    mean_forecast = pd.concat(forecasts, axis=1).mean(axis=1)\n",
    "\n",
    "    # Weighted Forecast (equal weights for simplicity)\n",
    "    weighted_forecast = pd.concat(forecasts, axis=1).apply(np.mean, axis=1)\n",
    "\n",
    "    # Store Aggregated Forecasts\n",
    "    aggregated_forecasts[f'{futures_col}_Mean'] = mean_forecast\n",
    "    aggregated_forecasts[f'{futures_col}_Weighted'] = weighted_forecast\n",
    "\n",
    "print(aggregated_forecasts)\n",
    "#### need to todo a rollign exercised. \n",
    "\n",
    "\n",
    "# Step 3: Visualize Aggregated Forecast\n",
    "plt.figure(figsize=(8, 5))\n",
    "for futures_col in forecast_dict.keys():\n",
    "    plt.plot(aggregated_forecasts[f'{futures_col}_Mean'], label=f'{futures_col} Mean Forecast')\n",
    "    plt.plot(aggregated_forecasts[f'{futures_col}_Weighted'], label=f'{futures_col} Weighted Forecast')\n",
    "\n",
    "plt.axvline(x=forecast_index[0], color='gray', linestyle='--', label='Forecast Start')\n",
    "plt.title('Aggregated Forecast Comparison')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display Results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### -----------------------------------------------------------------------------------------------\n",
    "\n",
    "### clean up\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "def cleanup_environment():\n",
    "    # List all variables in the global scope\n",
    "    globals_copy = globals().copy()\n",
    "    \n",
    "    # Delete DataFrames specifically (and optionally other variables)\n",
    "    for var_name, var_value in globals_copy.items():\n",
    "        if isinstance(var_value, pd.DataFrame):  # Checks if it's a DataFrame\n",
    "            print(f\"Deleting DataFrame: {var_name}\")\n",
    "            del globals()[var_name]\n",
    "    \n",
    "    # Trigger garbage collection\n",
    "    gc.collect()\n",
    "    print(\"Environment cleaned!\")\n",
    "\n",
    "# Call the cleanup function\n",
    "cleanup_environment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
